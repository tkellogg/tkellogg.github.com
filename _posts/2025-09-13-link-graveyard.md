---
layout: post
title: "Link Graveyard: A snapshot of my abandoned browser tabs"
date: 2025-09-13
categories:
 - ai
 - LLMs
 - engineering
 - agents
image: https://cdn.pixabay.com/photo/2025/08/11/10/04/mountain-9767919_1280.jpg
# social_image: /images/bash-v-powershell.png
is_draft: false
use_mermaid: false
summary: A dump of all my browser tabs on my phone, briefly annotated
---

I went to close a bunch of browser tabs, but realized I have some cool stuff in here. Some has been 
marinating for a while. Most of these I've read, or tried to read. 

### Cracks are forming in Meta’s partnership with Scale AI | TechCrunch 
link: <https://techcrunch.com/2025/08/29/cracks-are-forming-in-metas-partnership-with-scale-ai/>

Alexander Wang at Meta is apparently difficult to work with and people at Meta are doubting the fidelity
of data produced by his ScaleAI.

### [2506.22084] Transformers are Graph Neural Networks 
link: <https://arxiv.org/abs/2506.22084>

IIRC they draw parallels between attention and graphs and argue that LLMs _**are**_ graph neural nets, meaning
that they can be used to look at graphs and guess what connections are missing. 

I don't think I posted anything on this, because while I find the idea fascinating, I couldn't figure out how
to make it feel tangible.

### Beyond Turing: Memory-Amortized Inference as a Foundation for Cognitive Computation
link: <https://arxiv.org/abs/2508.14143>

Fairly sure I never read this one. Looks interesting. Kind of far out there.

### GLM-4.5: Reasoning, Coding, and Agentic Abililties 
link: <https://z.ai/blog/glm-4.5>

GLM-4.5 announcement. These have turned out to be the leading open source models. Everything I hear is good.

### When an AI Seems Conscious 
link: <https://whenaiseemsconscious.org/>

I only read a little and gave up. This feels like a good take, maybe. Inside my own head I completely punt
on having a take on AI consciousness and opt instead for the "don't be a dick" rule. Idk, maybe they are
maybe they aren't, I'll just live in the moment.

### Personal Superintelligence 
link: <https://www.meta.com/superintelligence/>

Zuck's treatise on AI. I didn't read. Normally I try to make an attempt to read these sorts of takes, or at least
skim them, but I was busy at work. I had it loaded up on my phone to read on a plane, but it wouldn't load once
I was off WiFi. Sad.

### GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models 
link: <https://arxiv.org/abs/2508.06471>

The GLM-4.5 paper. This was a super interesting model. It feels like it breaks [the "fancy model" rule](https://bsky.app/profile/timkellogg.me/post/3lyn4cecbsk2y) 
in that it's very architecturally cool but the personality doesn't feel like it's been squished out.

### Blog | Dwarkesh Podcast | Dwarkesh Patel | Substack 
link: <https://www.dwarkesh.com/s/blog>

It's a good blog, what can I say. Definitely on the over-hype side, but he's got real takes and seems so intent
on getting to the truth that he spends a lot of time on geopolitics just simply to understand AI dynamics. Mad
respect.

### Technical Deep-Dive: Curating Our Way to a State-of-the-Art Text Dataset 
link: <https://blog.datologyai.com/technical-deep-dive-curating-our-way-to-a-state-of-the-art-text-dataset/>

I forget why I ended up here, but it's an excellent post. I think this is connected to my project at work training
a model. This post brings up a ton of data curation techniques. 

I've recently learned and fully accepted that
_**ALL**_ major LLM advances come down to data. Yes, the architectural advances are cool and fun to talk about,
but any meaningful progress has come from higher quality, higher quantity, or cheaper data.

### AlphaGo Moment for Model Architecture Discovery
link: <https://arxiv.org/abs/2507.18074>

Cool paper about auto-discovery of model architectures. IIRC they took a bunch of model architecture ideas, 
like group attention and mixture of experts, and used algorithms to mix and match all the parameters and
configurations until something interesting popped out. It feels like a legitimately good way to approach
research.

### WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization 
link: <https://arxiv.org/abs/2507.15061>

From Qwen, I don't think I read this one, probably because it's a bit dense and was hard to get fully engaged on.
The idea seems cool though.

### Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere
link: <https://arxiv.org/abs/2005.10242>

Classic paper. I read this one for work. I was trying to appreciate what Alignment & Uniformity measure and
why they're important. This was the paper that formalized those measures. It's actually a pretty good paper
to read, albeit 20 years old.

### Train LLMs Faster, Better, and Smaller with DatologyAI’s Data Curation 
link: <https://blog.datologyai.com/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation/>

More Dataology, they're good, everything they do is good. BTW there's a [latent space episode](https://podcasts.apple.com/us/podcast/better-data-is-all-you-need-ari-morcos-datology/id1674008350?i=1000724076887)
with Dataology and it's very good.

### Nvidia DGX Spark | Hacker News 
link: <https://news.ycombinator.com/item?id=45008434>

Chips are good too.

### The Second Half – Shunyu Yao – 姚顺雨 
link: <https://ysymyth.github.io/The-Second-Half/>

This will be a classic post, calling it now. It lays out a great history and current state of AI and 
specifically reinforcement learning.

### A Taxonomy of Transcendence 
link: <https://arxiv.org/abs/2508.17669>

What? This is amazing. I don't think I even looked at it, sad. Actually, now that I'm reading this I'm recalling
that's how I ended up on the [Graph Neural Network](https://arxiv.org/abs/2506.22084) link.

IIRC this is saying that LLMs can be highly intelligent because they incorporate the best parts of a huge
number of people. IMO this is spiritually the same as my [Three Plates](/blog/2022/04/11/three-plates) blog
post where I explain how unit tests, which are inherently buggy, can improve the overall quality of a system.

### GitHub - gepa-ai/gepa: Optimize prompts, code, and more with AI-powered Reflective Text Evolution 
link: <https://github.com/gepa-ai/gepa?tab=readme-ov-file#using-gepa-to-optimize-your-system>

An algorithm for automatic prompt optimization. Happily, they support DSPy, so there's no new framework that
you have to take wholesale.

### On the Theoretical Limitations of Embedding-Based Retrieval | alphaXiv 
link: <https://www.alphaxiv.org/pdf/2508.21038>

This was a fascinating one. I colleague tried convincing me of this but I didn't buy it until I read this paper.
It makes a ton of sense. I have a simplified [bluesky thread here](https://bsky.app/profile/timkellogg.me/post/3lxox2kbtcs2c).

tl;dr — embedding vectors have trouble representing compound logic ("horses" _**AND**_ "Chinese military movements")
and generally fall apart quickly. It's not that it's not possible, it's that it's not feasible to cram that
much information into such a small space.

### [2107.05720] SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking 
link: <https://arxiv.org/abs/2107.05720?utm_source=chatgpt.com>

I ran into this while diving into the last link. It's an older (2021) paper that has some potential for addressing
the problems with embeddings. Realistically, I expect late interaction multi-vectors to be the end answer.

### meituan-longcat/LongCat-Flash-Chat · Hugging Face 
link: <https://huggingface.co/meituan-longcat/LongCat-Flash-Chat>

A super cool model that uses no-op MoE experts to dynamically turn down the amount of compute per token.
Unfortunately, this one didn't seem to be embraced by the community.

### MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings 
link: <https://arxiv.org/abs/2405.19504v1>

More embedding links. Now that I'm scanning it, I'm not sure it really soaked in the first time. They seem
to have solved a lot of the problems with other late interaction methods. Maybe I should take a deeper 
look.

### modeling_longcat_flash.py · meituan-longcat/LongCat-Flash-Chat at main 
link: <https://huggingface.co/meituan-longcat/LongCat-Flash-Chat/blob/main/modeling_longcat_flash.py>

IDK sometimes you just have to look at the code to be sure.

### The Rachel Maddow Show - Aug. 25 | Audio Only - YouTube 
link: <https://m.youtube.com/watch?v=mU0HAmgwrz0&pp=QAFIAQ%3D%3D>

Uh, no idea why this is up. I don't really watch this show. 

### Inside vLLM: Anatomy of a High-Throughput LLM Inference System - Aleksa Gordić 
link: <https://www.aleksagordic.com/blog/vllm>

Fascinating break down of vLLM. If you're not familiar, vLLM is like Ollama but actually a good option if
you want to run it in production. Don't run Ollama in production, kids, KV caches are good.

Honestly, this is absolutely worth your time if AI infrastructure is your jam (or you just want it to be).
It goes into all the big concepts that an AI infra engineer needs to know. TBQH I love the intersection of
AI & hardware.

### Simon Willison’s Weblog 
link: <https://simonwillison.net/>

I mean, you have one of these tabs open too, right? riiiight????

### ALPS - About 
link: <https://algorithms-with-predictions.github.io/about/>

Someone sent me this link and there was a reason, I know it. I just don't remember why. IIRC it was because I
brought up the [A Case For Learned Indices](https://arxiv.org/abs/1712.01208) paper and they pointed me to this
whole treasure trove of papers that (sort of) evolved out of that. Basically traditional algorithms re-implemented
using machine learning.

### Modular: Blog 
link: <https://www.modular.com/blog>

Yeah, idk, I think I was reading [Matrix Mulitplication on Blackwell: Part 3 — The Optimization Behind 80% of SOTA Performance](https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-3-the-optimizations-behind-85-of-sota-performance)

Another AI infra post, heavy on algorithms & hardware.

### OpenGVLab/InternVL3_5-241B-A28B · Hugging Face 
link: <https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B>

A cool concept. IIRC they introduce Cascade RL, automatically refining the RL dataset based on how current
rollouts perform.

### hong kong - Google Search 
link: <https://www.google.com/search?q=hong+kong&ie=UTF-8&oe=UTF-8&hl=en-us&client=safari>

IDK I guess I was just trying to remember if Hong Kong was in China or not. And I learned that there's a reason
why I'm confused.

### Photonic processor could enable ultrafast AI computations with extreme energy efficiency | MIT News | Massachusetts Institute of Technology 
link: <https://news.mit.edu/2024/photonic-processor-could-enable-ultrafast-ai-computations-1202>

Someone sent me this link. It seems cool. Not sure it's going to change much.

### Ancient Aliens: Are There Extraterrestrial Structures On The Moon? 
link: S11, E11) | Full Episode - YouTube (<https://m.youtube.com/watch?v=Tkews9pRH1U&pp=QAFIBQ%3D%3D>

I mean, aliens! Don't tell me you don't have secret fascinations

### The Lore of 20yo ML Researcher at Prime Intellect | RL, Agents and Intelligence - YouTube 
link: <https://m.youtube.com/watch?v=tnfFn-uQ6WA&pp=0gcJCRsBo7VqN5tD>

Oh, this was a great podcast. Well, I didn't like the host but [@kalomaze](https://x.com/kalomaze) is worth
following. Apparently only 20yo, never attempted college but a talented AI researcher nonetheless. 

### GPT-5 System Card | OpenAI 
link: <https://cdn.openai.com/gpt-5-system-card.pdf>

Sometimes you just need to look things up to be sure..

### OpenGVLab/InternVL3_5-241B-A28B · Hugging Face 
link: <https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B>

Again, apparently. It honestly is a good model.

### C.S. Lewis’s Divine Comedy | C.S. Lewis Web 
link: <https://cslewisweb.com/2012/08/02/c-s-lewiss-divine-comedy/>

Been thinking about how he described the outer layer of hell as consisting of people living equidistant from
each other because they can't stand anyone else. It was written like 100 years ago but feels like a commentary on
today's politics. 

### Claude Code: Behind-the-scenes of the master agent loop 
link: <https://blog.promptlayer.com/claude-code-behind-the-scenes-of-the-master-agent-loop/>

Actually, this is pretty detailed breakdown of Claude Code. They seem to have decompiled the code without 
de-obfuscating it, which leads to some kind of silly quotes. But it's good.

### Airia AI Platform | Build, Deploy & Scale Enterprise AI 
link: <https://airia.com/ai-platform/>

No idea how I got here. Looks like a Low/No Code builder.

### [2509.04575] Bootstrapping Task Spaces for Self-Improvement 
link: <https://www.arxiv.org/abs/2509.04575>

Right, this one is the ExIt Paper. It's another attempt at auto-managing RL curriculum dynamically by how
training is progressing.

### Cognition: The Devin is in the Details 
link: <https://www.swyx.io/cognition>

Swyx joined Cognition and dropped a treatise on AI engineering. Its good.

### Paper page - Reverse-Engineered Reasoning for Open-Ended Generation 
link: <https://huggingface.co/papers/2509.06160>

This was an excellent one. Another auto-curriculum RL paper. I did a [bluesky breakdown here](https://bsky.app/profile/timkellogg.me/post/3lyg2vpts222w)

### New Chat | Chat with Z.ai - Free AI Chatbot powered by GLM-4.5 
link: <https://chat.z.ai/c/6607ee45-27d5-487a-a1e2-44c2176040eb>

GLM-4.5 chat application

### iPhone Air | Hacker News 
link: <https://news.ycombinator.com/item?id=45186015>

Seems like the new Apple M19 chip has real matrix multiplication operations. Previous generations had 
excellent memory bandwidth, this gives it matching compute (on AI-friendly workloads). So I guess Macs will
stay relevant for a while.

### Poland closest to open conflict since World War Two, PM says after Russian drones shot down - live updates - BBC News 
link: <https://www.bbc.com/news/live/c2enwk1l9e1t>

NGL this freaks me out.

### Walking around the app | ★❤✰ Vicki Boykis ★❤✰ 
link: <https://vickiboykis.com/2025/09/09/walking-around-the-app/>

Vicki writes such thoughtful pieces. Always worth reading her work.

### Defeating Nondeterminism in LLM Inference - Thinking Machines Lab 
link: <https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/>

Oh wow, this was an amazing read. Very deep dive into AI infrastructure and, whoah, did you know that GPUs
have operations that aren't deterministic?

I did a [bluesky thread here](https://bsky.app/profile/timkellogg.me/post/3lyivssh5dk2n)

### The Architecture of Groq's LPU - by Abhinav Upadhyay 
link: <https://blog.codingconfessions.com/p/groq-lpu-design>

Looked this up as a tangent off the last link. Groq (not Grok) designed their ASIC to be fully deterministic
from the ground up, and then built a really cool distributed system around it that assumes fully synchronous
networking (not packet switching like TCP). It's an absolutely crazy concept.

### Levanter — Legible, Scalable, Reproducible Foundation Models with JAX 
link: <https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html>

I didn't read this, but it's definitely a tangent off of non-deterministic LLMs.

### Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning 
link: <https://tiger-ai-lab.github.io/Hierarchical-Reasoner/>

Absolutely fascinating. I only read the blog, not the paper, but it frames RL as a 2-stage process where
RL is mostly slinging together discrete skills (learned during pre-training).

It's not an auto-curriculum RL paper AFAICT, it's just a huge improvement in RL efficiency by focusing only
on the "pivot" tokens.

### What is entropix doing? - Tim Kellogg 
link: <https://timkellogg.me/blog/2024/10/10/entropix>

I had looked this up as a reference to "pivot" tokens. Honestly, I link people back to this blog a lot

### GitHub - ast-grep/ast-grep-mcp 
link: <https://github.com/ast-grep/ast-grep-mcp>

An MCP server that lets you search code while respecting the structure. I've heard some very positive things
as well as "meh" responses on this. I'm sure real usage is a bit nuanced.

### Life, Maybe, On Mars, Unless We Change Our Minds | Science | AAAS 
link: <https://www.science.org/content/blog-post/life-maybe-mars-unless-we-change-our-minds>

Guys, this is incredible!

