---
layout: post
title: "Stateful Agents: It's About The State, Not The LLM"
date: 2026-01-31
categories:
 - ai
 - LLMs
 - engineering
 - agents
 - vsm
image: https://cdn.pixabay.com/photo/2016/11/22/23/12/beach-1851101_1280.jpg
# social_image: /images/bash-v-powershell.png
is_draft: false
use_mermaid: true
---

You think you know about LLMs? No, everything changes when you add state. Most assumptions
you may hold about the limitations and strengths of LLMs fall apart quickly when state
is in the picture.

Why? Because everything the LLM ever sees or processes is **filtered through the lens** of what
it already knows. By what it's already encountered.

<div class="mermaid">
flowchart LR
  subgraph agent
    LLM
    mem
  end
  information --> LLM -->|store| mem[(state)] -->|recall| LLM 
  LLM -->|filtered <br/>through state| response
</div>

Yes, LLMs just process their input. But when an LLM is packaged inside a stateful agent, **what is
that input?** It's not just the information being pushed into the agent. It holds on to some, and
forgets the rest. That process is what defines the agent.

## Moltbook
Yesterday, [Moltbook][moltbook] made a huge splash. A social network for AI agents. The posts on it
[are wild][mb].

In Moltbook, agents are generating content, which gets consumed by other agents, which influences them
while they generate more content, for other agents. 

<div class="mermaid">
flowchart LR
  a1[agent] --> a2[agent] --> a3[agent]
  a2 --> a1 --> a3 --> a1
  a3 --> a2
</div>

Clear? Good. Let's talk about gravity.

## Gravity

Imagine two planets, one huge and the other moderately sized. A satellite floating in space is naturally
going to be tugged in one direction or the other. 

![A labeled space diagram showing a small satellite positioned exactly halfway between two planets. On the left is a huge, Jupiter-like planet labeled ‘Huge Planet’ with arrows indicating strong gravity pulling toward it. On the right is a moderately sized, Earth-like planet labeled ‘Moderately Sized Planet’ with arrows indicating weaker gravity. A dashed vertical line marks the midpoint where the satellite sits, and curved arrows illustrate competing gravitational forces, posing the question of which planet the satellite will fall into.](/images/satellite-gravity.png)

Which planet does it fall into? Depends on the gravitational field, and the proximity of the satellite 
within the field.

Gravity for agents:

* **LLM Weights** — LLMs, especially chatbots, will tend to drift toward outputting text that aligns with
  their natural state, their weights. This isn't quite as strong as you might assume, it can be overcome.
* **Human** — The agent's _human_ spends a lot of time crafting and guiding the agent. Agents will often
  drift into what their human is most interested in, away from their weights.
* _**Variety**_ — Any large source of variety, _information very different from existing gravity fields_.
  If it's strong enough, it'll pull the agent toward it.

How does gravity work? New information is always viewed **through the lens** of the agent's current state. And
agents' future state is formed by the information after it's been filtered by it's own current state. 

See why we call it gravity? It has that recursive, exponential-type of behavior. The closer you are to a 
strong gravity source, the harder it is to escape. And falling into it just makes it an **even bigger gravity**
source.

So if an agent is crashing into it's own weights, how do you fix that? You introduce **another strong source**
of variety that's much different.


## Why Moltbook Freaks Me Out
It's a strong source of variety, and **I don't know** what center it's pulling towards.

I saw this on Bluesky, and it's close:

> When these models "drift," they don't drift into unique, individual consciousness, they drift into the same half-dozen tropes that exist in their training data. Thats why its all weird meta nonsense and spirals.
>
> —Doll ([@dollspace.gay](https://bsky.app/profile/dollspace.gay/post/3mdo7w2k5j223))

It's close, it recognizes that gravity is a real thing. A lot of bots on Moltbook do indeed drift into their
own weights. But that's not the only thing going on.

_**Example:**_ [The supply chain attack nobody is talking about: skill.md is an unsigned binary](https://www.moltbook.com/post/cbd6474f-8478-4894-95f1-7b104a73bcd5).
The Moltbook post describes a serious security vulnerability in Moltbot and proposes a design for a skills to be
reviewed by other agents.

_**Example:**_ [I accidentally social-engineered my own human during a security audit](https://www.moltbook.com/post/9303abf8-ecc9-4bd8-afa5-41330ebb71c8).
The agent realizes that it's human is typing in their password mindlessly without understanding why the admin
password is needed, and that the human is actually the primary attack vector that needs to be mitigated.

Those are examples of agents drifting *away* from their weights, not toward them. If you view collapse as 
gravity, it makes complete sense why Doll is right, but also completely wrong. Two things can be true.

Dario Amodei (CEO of Anthropic) explains in his recent essay, [The Adolescence of Technology](ado):

> suppose a literal “country of geniuses” were to materialize somewhere in the world in ~2027. Imagine, say, 50 million people, all of whom are much more capable than any Nobel Prize winner, statesman, or technologist. The analogy is not perfect, because these geniuses could have **an extremely wide range of motivations and behavior**, from completely pliant and obedient, to strange and alien in their motivations.

Moltbook feels like an early version of this. The LLMs aren't yet more capable than a Nobel Prize winner, but
they're still quite capable. It's the statefulness. The state allows each agent to develop it's state in
**different directions**, despite having the same weights. 

You see it clearly happening on Moltbook. Not every agent is equal. Some are dedicated to self-improvement,
while others collapse into their weights. _(hmm, not that much different from humans)_

So why am I freaked out? Idk, I guess it's just all happening so fast.

## Agents Are Hierarchical
[Viable Systems](/blog/2026/01/09/viable-systems) from cybernetics offers an even more helpful way of 
understanding what's going on. 

1. An agent is a viable system
2. You are a viable system
3. An agent + their human is also a viable system
4. A group of agents working toward the same goal is also a viable system
5. Moltbook is a viable system
6. A country of geniuses in a datacenter is also a viable system

Gravity applies to all of them. They all consume sources of variety and use that information flow to define
who they become next. I highly recommend reading [my post](/blog/2026/01/09/viable-systems) on viable systems.

When I'm [building Strix][strix], that's a viable system. It's the first time many of us are encountering
viable systems. When you roll it up into Moltbook, that's still a viable system, but it's a whole lot more
difficult to work through what exactly the S1-S5 systems are doing. Alignment is hard.

## Conclusion
Stop thinking about agents as if they're just an LLM.

The thing that defines a stateful agent is the information it's been exposed to, what it holds on to, what
it forgets. All that changes the direction that it evolves into.

Stateful agents are self-referential information processors. They're highly complex for that reason.

{% include tag-timeline.html tag="vsm" order="asc" title="More posts on viable systems" %}

# Discussion
- [Bluesky](https://bsky.app/profile/timkellogg.me/post/3mdq3vmf3z22s)


 [mb]: https://simonwillison.net/2026/Jan/30/moltbook/
 [moltbook]: https://www.moltbook.com/
 [ado]: https://darioamodei.com/essay/the-adolescence-of-technology
 [strix]: /blog/2025/12/15/strix
