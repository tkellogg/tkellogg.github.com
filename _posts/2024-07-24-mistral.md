---
layout: post
title: "Mistral: Are LLMs Commodities Now?"
date: 2024-07-24
categories:
 - ai
 - strategy
image: https://cdn.pixabay.com/photo/2014/08/26/21/29/padlock-428549_960_720.jpg
is_draft: false
use_mermaid: false
summary: 
  Mistral 2 Large is out, and it's right up there with GPT-4o, ...and Llama 3.1, and Claude Sonnet 3.5, and...yeah, there's a lot of them. 
  These "Frontier Models" are starting to look more like commodities. And with that shift, we need 
  to adjust AI strategy to match. There's strong arguments to make for using an operator that
  doesn't also train models. Read more!
---

[Mistral 2 Large is out][mistral], and it's right up there with GPT-4o, ...and Llama 3.1, 
and Claude Sonnet 3.5, and...yeah, there's a lot of them.

They call them "Frontier Models", but frankly the frontier is getting quite crowded. At 
some point GPT-5 will be released, and presumably that will be a fully new level of 
capabilities. But that's not expected for [1-2 years][cnn].

So this is what we got. If you're building an AI strategy, this is the level of 
capabilities you have to work with. The trade-off decisions look more like comodity trade-offs:

* Cost
* Availability (open source? API pricing? on my cloud?)
* Operator trustworthiness

Some do better on math. Some on multi-language capabilities. But in general, any of these
models will be okay to base your corporate AI strategy.

# Builders Are Bad Operators
The companies building LLMs — OpenAI, Mistral, Anthropic, etc. — all have incentives that
are quite contrary to being a good operator. Mainly that last point, operator trustworthiness.
In order to compete at the next level (GPT-5), they need lots of data. Mountains of it.
And a lot of it is coming from ChatGPT sessions and API requests.

Any CISO should rightly look at the OpenAIs, the Anthropics, and the Mistrals of the world
with skepticism. "How are you going to acquire enough data to keep up with the next leap,
without endangering my security?"

So use an operator that just operates. No training.

* OpenAI API: Bad
* Hosted ChatGPT: Worse
* Azure AI, AWS Bedrock, Google Cloud: Better
* [Nvidia][nv], [Groq][groq]: Great!

Those last two are suppliers of AI chips. Their offerings are mainly for demonstrating how great
their chips are, so you can count on the cost & latency to steadily go down.

Until GPT-5, ✌️

 [mistral]: https://mistral.ai/news/mistral-large-2407/
 [cnn]: https://www.ccn.com/news/technology/chatgpt5-release-timing-details-in-full-openai/
 [nv]: https://www.nvidia.com/en-us/ai-data-science/products/nemo/
 [groq]: https://groq.com/
