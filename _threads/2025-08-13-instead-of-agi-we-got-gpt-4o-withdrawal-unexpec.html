---
layout: thread
title: "instead of AGI we got.. gpt-4o withdrawal"
date: 2025-08-13 00:11:55 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lwak2mr6ys2w
likes: 25
reposts: 1
post_count: 2
summary: "instead of AGI we got.. gpt-4o withdrawal  unexpectedly (or maybe expectedly), users formed a psychological bond with 4o and ripping it away seems to ..."
similar:
  - url: "/threads/2025-10-30-gpt-oss-safeguard-20b-120b-a-pair-of-open-weigh/"
    title: "gpt-oss-safeguard 20b & 120b"
  - url: "/threads/2025-07-12-openai-open-weights-model-is-being-delayed/"
    title: "OpenAI open weights model is being delayed"
  - url: "/threads/2025-05-24-reading-claude-4-system-card-and-this-is-2001-a/"
    title: "reading Claude 4 system card and... this is 2001 a Space Odyssey. lots of sel..."
---
<div class="thread-post">
<div class="post-text">instead of AGI we got.. gpt-4o withdrawal<br><br>unexpectedly (or maybe expectedly), users formed a psychological bond with 4o and ripping it away seems to have cut them deep<br><br>where to go from here?</div>
<div class="post-images multiple">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreietxq6y425dak6u3mhh6asw5msxr5nyfzm2vktkfgct4cbokl3qfy@jpeg" alt="Sam Altman
@sama
If you have been following the GPT-5 rollout, one thing you might be noticing is how much of an attachment some people have to specific Al models. It feels different and stronger than the kinds of attachment people have had to previous kinds of technology (and so suddenly deprecating old models that users depended on in their workflows was a mistake).
This is something we've been closely tracking for the past year or so but still hasn't gotten much mainstream attention (other than when we released an update to GPT-4o that was too sycophantic).
(This is just my current thinking, and not yet an official OpenAl position.)
People have used technology including Al in self-destructive ways; if a user is in a mentally fragile state and prone to delusion, we do not want the Al to reinforce that. Most users can keep a clear line between reality and fiction or role-play, but a small percentage cannot. We value user freedom as a core principle, but we also feel responsible" class="post-image" loading="lazy">
<div class="image-alt">Sam Altman
@sama
If you have been following the GPT-5 rollout, one thing you might be noticing is how much of an attachment some people have to specific Al models. It feels different and stronger than the kinds of attachment people have had to previous kinds of technology (and so suddenly deprecating old models that users depended on in their workflows was a mistake).
This is something we've been closely tracking for the past year or so but still hasn't gotten much mainstream attention (other than when we released an update to GPT-4o that was too sycophantic).
(This is just my current thinking, and not yet an official OpenAl position.)
People have used technology including Al in self-destructive ways; if a user is in a mentally fragile state and prone to delusion, we do not want the Al to reinforce that. Most users can keep a clear line between reality and fiction or role-play, but a small percentage cannot. We value user freedom as a core principle, but we also feel responsible</div>
</div>
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreie74gxuzt2pyipmxm7hhj2gzg43c7ldws4bd4iegvq3geix6fbd6y@jpeg" alt="as a core principle, but we also feel responsible in how we introduce new technology with new risks.
Encouraging delusion in a user that is having trouble telling the difference between reality and fiction is an extreme case and it's pretty clear what to do, but the concerns that worry me most are more subtle. There are going to be a lot of edge cases, and generally we plan to follow the principle of &quot;treat adult users like adults&quot; , which
in some cases will include pushing back on users to ensure they are getting what they really want.
A lot of people effectively use ChatGPT as a sort of therapist or life coach, even if they wouldn't describe it that way. This can be really good! A lot of people are getting value from it already today.
If people are getting good advice, leveling up toward their own goals, and their life satisfaction is increasing over years, we will be proud of making something genuinely helpful, even if they use and rely on ChatGPT a lot. If, on the other hand, users have a relationship with ChatGPT where they think they feel better after talking but they're unknowingly nudged away from their" class="post-image" loading="lazy">
<div class="image-alt">as a core principle, but we also feel responsible in how we introduce new technology with new risks.
Encouraging delusion in a user that is having trouble telling the difference between reality and fiction is an extreme case and it's pretty clear what to do, but the concerns that worry me most are more subtle. There are going to be a lot of edge cases, and generally we plan to follow the principle of &quot;treat adult users like adults&quot; , which
in some cases will include pushing back on users to ensure they are getting what they really want.
A lot of people effectively use ChatGPT as a sort of therapist or life coach, even if they wouldn't describe it that way. This can be really good! A lot of people are getting value from it already today.
If people are getting good advice, leveling up toward their own goals, and their life satisfaction is increasing over years, we will be proud of making something genuinely helpful, even if they use and rely on ChatGPT a lot. If, on the other hand, users have a relationship with ChatGPT where they think they feel better after talking but they're unknowingly nudged away from their</div>
</div>
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreig5alp627ruhriqiiuvf5ue7ozcem3r6yt6v5hip74ltmfd375rvi@jpeg" alt="making something genuinely helpful, even it they use and rely on ChatGPT a lot. If, on the other hand, users have a relationship with ChatGPT where they think they feel better after talking but they're unknowingly nudged away from their longer term well-being (however they define it), that's bad. It's also bad, for example, if a user wants to use ChatGPT less and feels like they cannot.
I can imagine a future where a lot of people really trust ChatGPT's advice for their most important decisions. Although that could be great, it makes me uneasy. But I expect that it is coming to some degree, and soon billions of people may be talking to an Al in this way. So we (we as in society, but also we as in OpenAl) have to figure out how to make it a big net positive.
There are several reasons I think we have a good shot at getting this right. We have much better tech to help us measure how we are doing than previous generations of technology had. For example, our product can talk to users to get a sense for how they are doing with their short-and long-term goals, we can explain sophisticated and nuanced issues to our models, and much more." class="post-image" loading="lazy">
<div class="image-alt">making something genuinely helpful, even it they use and rely on ChatGPT a lot. If, on the other hand, users have a relationship with ChatGPT where they think they feel better after talking but they're unknowingly nudged away from their longer term well-being (however they define it), that's bad. It's also bad, for example, if a user wants to use ChatGPT less and feels like they cannot.
I can imagine a future where a lot of people really trust ChatGPT's advice for their most important decisions. Although that could be great, it makes me uneasy. But I expect that it is coming to some degree, and soon billions of people may be talking to an Al in this way. So we (we as in society, but also we as in OpenAl) have to figure out how to make it a big net positive.
There are several reasons I think we have a good shot at getting this right. We have much better tech to help us measure how we are doing than previous generations of technology had. For example, our product can talk to users to get a sense for how they are doing with their short-and long-term goals, we can explain sophisticated and nuanced issues to our models, and much more.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>25</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">yaccine might actually have a point here</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreias6y2lth7cjxthuyaiklsz6kafphctedtw5tity6vaujtlzph4hi@jpeg" alt="roon
@tszzl • 23h
the long tail of GPT-4o interactions scares me, there are strange things going on on a scale I didn't appreciate before the attempted deprecation of the model
Sam Altman
@sama • 1d
If you have been following the GPT-5 rollout, one thing you might be noticing is how much of an attachment some people have to specific Al models. It feels different and stronger than the kinds o... Show more
189
t7 119
2.8K
ihl 435K
roon
@tszzl • 23h
when you receive quite a few DMs asking you to bring back 4o and many of the messages are clearly written by 4o it starts to get a bit hair raising
96
t7 130
2K
Ill 276K
企
kache
@yacineMTB • 23h
that's the scary thing they weren't written by 4o
31
t75
331
ill 20K" class="post-image" loading="lazy">
<div class="image-alt">roon
@tszzl • 23h
the long tail of GPT-4o interactions scares me, there are strange things going on on a scale I didn't appreciate before the attempted deprecation of the model
Sam Altman
@sama • 1d
If you have been following the GPT-5 rollout, one thing you might be noticing is how much of an attachment some people have to specific Al models. It feels different and stronger than the kinds o... Show more
189
t7 119
2.8K
ihl 435K
roon
@tszzl • 23h
when you receive quite a few DMs asking you to bring back 4o and many of the messages are clearly written by 4o it starts to get a bit hair raising
96
t7 130
2K
Ill 276K
企
kache
@yacineMTB • 23h
that's the scary thing they weren't written by 4o
31
t75
331
ill 20K</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>10</span>
</div>
</div>