---
layout: thread
title: "Karpathy mentioned entropy collapse in LLMs, where they stop doing interestin..."
date: 2025-10-23 20:01:44 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3m3v5nk7uec2m
likes: 28
reposts: 2
post_count: 3
summary: "Karpathy mentioned entropy collapse in LLMs, where they stop doing interesting things because they just don’t have interesting things in their trainin..."
similar:
  - url: "/threads/2025-08-16-dear-gpt-5-im-pretty-sure-storagely-is-not-a-w/"
    title: "dear GPT-5, i’m pretty sure “storagely” is not a word, but that oddly makes a..."
  - url: "/threads/2025-08-21-im-coming-to-the-conclusion-that-most-programmers/"
    title: "i’m coming to the conclusion that most programmers are very bad at using LLMs..."
  - url: "/threads/2025-08-23-ive-gotten-the-opportunity-at-work-to-train-a-sma/"
    title: "i’ve gotten the opportunity at work to train a small LLM. my brain has been c..."
---
<div class="thread-post">
<div class="post-text">Karpathy mentioned entropy collapse in LLMs, where they stop doing interesting things because they just don’t have interesting things in their training data<br><br>tempting to think they just need randomness, but i think that would make it worse<br><br>they need memory & experience</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>28</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>2</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">i tend to prefer hiring people that are on their second career bc i think it solves some of the same entropy collapse that happens in LLMs <br><br>entropy is the opposite of randomness, it’s just a new source of dense signal</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>8</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">meh, by “entropy” i mean “low entropy”, i.e. the opposite of what i said. just vibe with it</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>7</span>
</div>
</div>