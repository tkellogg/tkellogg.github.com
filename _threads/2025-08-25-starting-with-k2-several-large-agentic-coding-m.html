---
layout: thread
title: "Starting with K2, several large “agentic coding” models weren’t trained as re..."
date: 2025-08-25 14:42:28 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lxaaccquz22j
likes: 29
reposts: 0
post_count: 2
summary: "Starting with K2, several large “agentic coding” models weren’t trained as reasoners:  - K2 - GLM-4.5 (current SOTA open weights) - Opus 4.1 benchmark..."
similar:
  - url: "/threads/2025-07-13-k2-is-the-first-im-aware-that-did-this-directly/"
    title: "K2 is the first i’m aware that did this, directly training on *thousands* of ..."
  - url: "/threads/2025-09-12-its-true-the-last-two-qwens-have-been-beautiful/"
    title: "it’s true, the last two Qwens have been beautiful works of art, until you tal..."
  - url: "/threads/2025-08-01-gpt5-is-such-a-great-coding-model/"
    title: "GpT5 iS sUcH a GrEaT cOdInG mOdEl"
---
<div class="thread-post">
<div class="post-text">Starting with K2, several large “agentic coding” models weren’t trained as reasoners:<br><br>- K2<br>- GLM-4.5 (current SOTA open weights)<br>- Opus 4.1 benchmarked in non-thinking mode<br><br>seems like convergence on “reasoning isn’t necessary for coding”, or maybe undesirable since it uses up context</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>29</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">meanwhile, GPT-5 is going the opposite direction</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>11</span>
</div>
</div>