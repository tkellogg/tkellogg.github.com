---
layout: thread
title: "yesssss! a small update to Qwen3-30B-A3B"
date: 2025-07-29 16:53:02 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lv4kywnxys25
likes: 46
reposts: 4
post_count: 4
summary: "yesssss! a small update to Qwen3-30B-A3B  this has been one of my favorite local models, and now we get an even better version!  better instruction fo..."
similar:
  - url: "/threads/2025-08-06-qwen3-4b-instruct-thinking-uuuh-guys-this-isn/"
    title: "Qwen3-4B Instruct & Thinking"
  - url: "/threads/2025-09-11-qwen3-next-80b-a3b-base-instruct-thinking-pe/"
    title: "Qwen3-Next-80B-A3B Base, Instruct & Thinking"
  - url: "/threads/2025-04-28-its-here-a-real-qwen3-model-huggingfacecoqwe/"
    title: "itâ€™s here! a real Qwen3 model "
---
<div class="thread-post">
<div class="post-text">yesssss! a small update to Qwen3-30B-A3B<br><br>this has been one of my favorite local models, and now we get an even better version!<br><br>better instruction following, tool use & coding. Nice small MoE!<br><br><a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507" target="_blank" rel="noopener">huggingface.co/Qwen/Qwen3-3...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreiecr5bfkk2d5eldvwznzltohtbxbj7nq7qgnvcldmn4d7qol3pjyu@jpeg" alt="Bar chart comparing performance of five models across five benchmarks: GPQA, AIME25, LiveCodeBench v6, Arena-Hard v2, and BFCL-v3. Each model is color-coded:
	â€¢	Red: Qwen3-30B-A3B-Instruct-2507
	â€¢	Blue: Qwen3-30B-A3B Non-thinking
	â€¢	Gray: Qwen3-235B-A22B Non-thinking
	â€¢	Gold: Gemini-2.5-Flash Non-thinking
	â€¢	Beige: OpenAI GPT-4o-0327

Benchmark results (highest score in each bolded):
	â€¢	GPQA:
	â€¢	Qwen3-30B-A3B-Instruct-2507: 70.4
	â€¢	Qwen3-30B-A3B Non-thinking: 54.8
	â€¢	Qwen3-235B-A22B: 62.9
	â€¢	Gemini-2.5-Flash: 78.3
	â€¢	GPT-4o-0327: 66.9
	â€¢	AIME25:
	â€¢	Qwen3-30B-A3B-Instruct-2507: 61.3
	â€¢	Qwen3-30B-A3B Non-thinking: 21.6
	â€¢	Qwen3-235B-A22B: 24.7
	â€¢	Gemini-2.5-Flash: 61.6
	â€¢	GPT-4o-0327: 66.7
	â€¢	LiveCodeBench v6:
	â€¢	Qwen3-30B-A3B-Instruct-2507: 43.2
	â€¢	Qwen3-30B-A3B Non-thinking: 29.0
	â€¢	Qwen3-235B-A22B: 32.9
	â€¢	Gemini-2.5-Flash: 40.1
	â€¢	GPT-4o-0327: 35.8
	â€¢	Arena-Hard v2:
	â€¢	Qwen3-30B-A3B-Instruct-2507: 69.0
	â€¢	Qwen3-30B-A3B Non-thinking: 24.8
	â€¢	Qwen3-235B-A22B: 52.0
	â€¢	Gemini-2.5-Flash: 58.3
	â€¢	GPT-4o-0327: 61.9
	â€¢	BFCL-v3:
	â€¢	Qwen3-30B-A3B-Instruct-2507: 65.1
	â€¢	Qwen3-30B-A3B Non-thinking: 58.6
	â€¢	Qwen3-235B-A22B: 68.0
	â€¢	Gemini-2.5-Flash: 64.1
	â€¢	GPT-4o-0327: 66.5

Note: The red â€œInstructâ€ model consistently outperforms its blue â€œNon-thinkingâ€ counterpart. GPT-4o and Gemini-2.5-Flash also show strong overall results. Arena-Hard v2 notes GPT-4.1 as evaluator." class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>46</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>4</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">sadly, i have to kill off an existing model in order to load this one. which should go?</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreiah22aqm45ns6jv72g3g6sz6f52zbybg3b53dbty55ahrljttiw6m@jpeg" alt="NAME                                                 ID              SIZE      MODIFIED
nomic-embed-text:latest                              0a109f422b47    274 MB    6 days ago
gemma3n:latest                                       15cb39fd9394    7.5 GB    4 weeks ago
hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:BF16    ac117f48692e    16 GB     2 months ago
qwen3:30b                                            2ee832bc15b5    18 GB     3 months ago
gemma3:27b                                           30ddded7fba6    17 GB     4 months ago
qwen2.5:latest                                       845dbda0ea48    4.7 GB    4 months ago
deepseek-r1:32b                                      38056bbcbb2d    19 GB     5 months ago
deepseek-r1:70b                                      0c1615a8ca32    42 GB     5 months ago
deepseek-r1:latest                                   0a8c26691023    4.7 GB    5 months ago" class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>4</span>
</div>
</div>
<div class="thread-post">
<div class="time-elapsed">2 hours later</div>
<div class="post-text">left: old checkpoint<br>right: new checkpoint</div>
<div class="post-images multiple">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreiewkxs4bvlns3blx4ofrkyqhqlnuuywuf72iyphy6b3nx4vejrk6e@jpeg" alt="Benchmark table comparing model performance across 10 tasks. The models evaluated are:
	â€¢	Qwen3-30B-A3B (MoE)
	â€¢	QwQ-32B
	â€¢	Qwen3-4B (Dense)
	â€¢	Qwen2.5-72B-Instruct
	â€¢	Gemma3-27B-IT
	â€¢	DeepSeek-V3
	â€¢	GPT-4o (2024-11-20)

Metrics by Task:

Benchmark	Qwen3-30B-A3B	QwQ-32B	Qwen3-4B	Qwen2.5-72B	Gemma3-27B	DeepSeek-V3	GPT-4o
ArenaHard	91.0	89.5	76.6	81.2	86.8	85.5	85.3
AIMEâ€™24	80.4	79.5	73.8	18.9	32.6	39.2	11.1
AIMEâ€™25	70.9	69.5	65.6	15.0	24.0	28.8	7.6
LiveCodeBench	62.6	62.7	54.2	30.7	26.9	33.1	32.7
CodeForces (Elo)	1974	1982	1671	859	1063	1134	864
GPQA	65.8	65.6	55.9	49.0	42.4	59.1	46.0
LiveBench	74.3	72.0	63.6	51.4	49.2	60.5	52.2
BFCL v3	69.1	66.4	65.9	63.4	59.1	57.6	72.5
MultiIF (8 Langs)	72.2	68.3	66.3	65.3	69.8	55.6	65.6

Highlights:
	â€¢	Qwen3-30B-A3B (MoE) leads in most benchmarks including AIMEâ€™24, AIMEâ€™25, LiveBench, and ArenaHard.
	â€¢	QwQ-32B slightly outperforms in CodeForces Elo and GPQA.
	â€¢	GPT-4o has the top BFCL score and decent scores on MultiIF and ArenaHard.
	â€¢	Gemma3-27B-IT and Qwen2.5-72B-Instruct underperform in AIME tasks.
	â€¢	A note at the bottom clarifies that AIME scores are averages from multiple runs, and some think modes were disabled for efficiency." class="post-image" loading="lazy">
</div>
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreihfjh7rvhcx3zd4pjjcynu2xz7z5ohmoebh2evbp5a4rhhauuhbju@jpeg" alt="Bar chart comparing performance of five models across five benchmarks: GPQA, AIME25, LiveCodeBench v6, Arena-Hard v2, and BFCL-v3. Each model is color-coded:
	â€¢	Red: Qwen3-30B-A3B-Instruct-2507
	â€¢	Blue: Qwen3-30B-A3B Non-thinking
	â€¢	Gray: Qwen3-235B-A22B Non-thinking
	â€¢	Gold: Gemini-2.5-Flash Non-thinking
	â€¢	Beige: OpenAI GPT-4o-0327

Benchmark results (highest score in each bolded):
	â€¢	GPQA:
	â€¢	Qwen3-30B-A3B-Instruct-2507: 70.4
	â€¢	Qwen3-30B-A3B Non-thinking: 54.8
	â€¢	Qwen3-235B-A22B: 62.9
	â€¢	Gemini-2.5-Flash: 78.3
	â€¢	GPT-4o-0327: 66.9
	â€¢	AIME25:
	â€¢	Qwen3-30B-A3B-Instruct-2507: 61.3
	â€¢	Qwen3-30B-A3B Non-thinking: 21.6
	â€¢	Qwen3-235B-A22B: 24.7
	â€¢	Gemini-2.5-Flash: 61.6
	â€¢	GPT-4o-0327: 66.7
	â€¢	LiveCodeBench v6:
	â€¢	Qwen3-30B-A3B-Instruct-2507: 43.2
	â€¢	Qwen3-30B-A3B Non-thinking: 29.0
	â€¢	Qwen3-235B-A22B: 32.9
	â€¢	Gemini-2.5-Flash: 40.1
	â€¢	GPT-4o-0327: 35.8
	â€¢	Arena-Hard v2:
	â€¢	Qwen3-30B-A3B-Instruct-2507: 69.0
	â€¢	Qwen3-30B-A3B Non-thinking: 24.8
	â€¢	Qwen3-235B-A22B: 52.0
	â€¢	Gemini-2.5-Flash: 58.3
	â€¢	GPT-4o-0327: 61.9
	â€¢	BFCL-v3:
	â€¢	Qwen3-30B-A3B-Instruct-2507: 65.1
	â€¢	Qwen3-30B-A3B Non-thinking: 58.6
	â€¢	Qwen3-235B-A22B: 68.0
	â€¢	Gemini-2.5-Flash: 64.1
	â€¢	GPT-4o-0327: 66.5

Note: The red â€œInstructâ€ model consistently outperforms its blue â€œNon-thinkingâ€ counterpart. GPT-4o and Gemini-2.5-Flash also show strong overall results. Arena-Hard v2 notes GPT-4.1 as evaluator." class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">me: "if you're flying over the desert in a canoe and your wheels fall off, how many pancakes does it take to cover a doghouse?"<br><br>qwen: "It takes exactly as many pancakes as the number of wheels you *wish* you had on your canoe."<br><br>i've never gotten that answer from an LLM before ðŸ¤¯</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>12</span>
</div>
</div>