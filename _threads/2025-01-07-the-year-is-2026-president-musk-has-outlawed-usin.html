---
layout: thread
title: "The year is 2026. President Musk has outlawed using proprietary model outputs..."
date: 2025-01-07 22:56:59 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lf6qotiots2x
likes: 21
reposts: 3
post_count: 2
summary: "The year is 2026. President Musk has outlawed using proprietary model outputs to train smaller models. Attorney General Sam Altman has been tasked wit..."
similar:
  - url: "/threads/2025-02-09-the-deepseek-effect-is-that-now-any-new-model-only/"
    title: "the deepseek effect is that now any new model only has to exceed R1 in order ..."
  - url: "/threads/2024-11-12-scaling-laws-for-precision-yes-llama-models-are/"
    title: "Scaling Laws for Precision"
  - url: "/threads/2025-11-07-notable-they-ripped-out-the-silicon-that-supports/"
    title: "notable: they ripped out the silicon that supports training"
---
<div class="thread-post">
<div class="post-text">The year is 2026. President Musk has outlawed using proprietary model outputs to train smaller models. Attorney General Sam Altman has been tasked with quashing illegal distilleries from creating illicit open source moonshine LLMs</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>21</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">but fr this is a great paper <a href="https://arxiv.org/abs/2408.16737" target="_blank" rel="noopener">arxiv.org/abs/2408.16737</a></div>
<a href="https://arxiv.org/abs/2408.16737" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">arxiv.org</div>
<div class="embed-title">Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling</div>
<div class="embed-description">Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-op...</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>6</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>