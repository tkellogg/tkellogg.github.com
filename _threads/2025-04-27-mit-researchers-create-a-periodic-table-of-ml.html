---
layout: thread
title: "MIT researchers create a “periodic table” of ML"
date: 2025-04-27 11:12:57 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lns4n7hxbk2s
likes: 38
reposts: 11
post_count: 2
summary: "MIT researchers create a “periodic table” of ML  “These spaces predict where algorithms should exist, but which haven’t been discovered yet.”  “We’re ..."
similar:
  - url: "/threads/2025-09-09-llms-predict-the-next-token-this-is-insane-rig/"
    title: "\"LLMs predict the next token\""
  - url: "/threads/2025-10-01-years-ago-data-scientist-was-a-phd-only-positio/"
    title: "years ago, “data scientist” was a PhD-only position at Big Tech, or just an a..."
  - url: "/threads/2025-07-19-openai-researcher-posts-on-x-not-a-blog-or-paper/"
    title: "openai researcher posts on X (not a blog or paper) about a model they have th..."
---
<div class="thread-post">
<div class="post-text">MIT researchers create a “periodic table” of ML<br><br>“These spaces predict where algorithms should exist, but which haven’t been discovered yet.”<br><br>“We’re starting to see ML as a system with structure that is a space we can explore rather than just guess our way through.”<br><br><a href="https://news.mit.edu/2025/machine-learning-periodic-table-could-fuel-ai-discovery-0423?utm_source=chatgpt.com" target="_blank" rel="noopener">news.mit.edu/2025/machine...</a></div>
<a href="https://news.mit.edu/2025/machine-learning-periodic-table-could-fuel-ai-discovery-0423?utm_source=chatgpt.com" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">news.mit.edu</div>
<div class="embed-title">“Periodic table of machine learning” could fuel AI discovery</div>
<div class="embed-description">After uncovering a unifying algorithm that links more than 20 common machine-learning approaches, MIT researchers organized them into a “periodic table of machine learning” that can help scientists co...</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>38</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>11</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">reminds me of LLaDA, where they explained that autoregression is equal to diffusion, thus you absolutely can do diffusion for text.<br><br>maybe another relationship to add to the table?<br><br><a href="https://timkellogg.me/blog/2025/02/17/diffusion" target="_blank" rel="noopener">timkellogg.me/blog/2025/02...</a></div>
<a href="https://timkellogg.me/blog/2025/02/17/diffusion" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">timkellogg.me</div>
<div class="embed-title">LLaDA: LLMs That Don't Gaslight You</div>
<div class="embed-description">A new language model uses diffusion instead of next-token prediction. That means the text it can back out of a hallucination before it commits. This is a big win for areas like law & contracts, where ...</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>7</span>
</div>
</div>