---
layout: thread
title: "The Case Against Multi-Agents"
date: 2025-06-15 11:48:44 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lrnfibxrhc2u
likes: 37
reposts: 5
post_count: 5
summary: "The Case Against Multi-Agents  Cognition (i.e. Devin) coins the term “context engineering”, successor to prompt engineering and argues that multi-agen..."
similar:
  - url: "/threads/2025-06-14-pretty-strong-argument-for-multi-agents-wwwanthr/"
    title: "pretty strong argument for multi-agents"
  - url: "/threads/2025-12-04-i-thought-this-was-a-generic-agent-product-for-int/"
    title: "i thought this was a generic agent product for interviewing people"
  - url: "/threads/2025-05-06-my-evolving-take-on-a2a-is-that-the-world-isnt-re/"
    title: "my evolving take on A2A is that the world isn't ready, and i'm not sure it ev..."
---
<div class="thread-post">
<div class="post-text">The Case Against Multi-Agents<br><br>Cognition (i.e. Devin) coins the term “context engineering”, successor to prompt engineering and argues that multi-agents don’t pass context effectively <br><br><a href="https://cognition.ai/blog/dont-build-multi-agents#applying-the-principles" target="_blank" rel="noopener">cognition.ai/blog/dont-bu...</a></div>
<a href="https://cognition.ai/blog/dont-build-multi-agents#applying-the-principles" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">cognition.ai</div>
<div class="embed-title">Cognition | Don’t Build Multi-Agents</div>
<div class="embed-description">Frameworks for LLM Agents have been surprisingly disappointing. I want to offer some principles for building agents based on our own trial & error, and explain why some tempting ideas are actually qui...</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>37</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>5</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">Anthropic’s take appears conflicting, but i do not think it is <br><br>for example, here Cognition introduces the idea of a “Context Compression LLM”<br><br>that’s a multi-agent<br><br>also, Anthropic’s whole point was that intelligence = compression<br><br><a href="https://bsky.app/profile/timkellogg.me/post/3lrkwipfdnc2k" target="_blank" rel="noopener">bsky.app/profile/timk...</a></div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>9</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">you also have to pay attention to the problems they’re approaching <br><br>Anthropic: research. They explicitly said LLMs are good at *highly parallelizable tasks*, i.e. don’t require context sharing<br><br>Cognition: code, where context is inherently shared</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>7</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">the Cognition post makes the point that *today’s LLMs* are bad at intelligently sharing context<br><br>in that, if i want to brief you in order to do a sub-task, i tell you all you need to know. LLMs have trouble knowing what’s important<br><br>Anthropic dwells in the future</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>4</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">regardless, you can get a very long way with agents & LLMs by simply reasoning about information flow<br><br>Anthropic focused more on compression, but didn’t say much on what they were doing to achieve it. Cognition’s take is less heady, but they’re saying the same thing</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>3</span>
</div>
</div>