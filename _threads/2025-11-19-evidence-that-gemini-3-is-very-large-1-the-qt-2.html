---
layout: thread
title: "Evidence that Gemini 3 is very large:"
date: 2025-11-19 12:43:08 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3m5ybq4osp22p
likes: 22
reposts: 1
post_count: 6
summary: "Evidence that Gemini 3 is very large:  1. the QT 2. Artificial analysis (image) quote: x.com/artificialan... report: artificialanalysis.ai/evaluations..."
similar:
  - url: "/threads/2025-06-17-gemini-25-tech-report-is-out-the-tech-report-go/"
    title: "Gemini 2.5 tech report is out!"
  - url: "/threads/2025-05-20-oh-wow-gemini-is-doing-is-doing-a-text-diffusion/"
    title: "oh wow, Gemini is doing is doing a text diffusion model"
  - url: "/threads/2025-11-13-gemini-3-on-par-with-experts-this-riveting-surpr/"
    title: "Gemini 3 on par with experts"
---
<div class="thread-post">
<div class="post-text">Evidence that Gemini 3 is very large:<br><br>1. the QT<br>2. Artificial analysis (image)<br>quote: <a href="https://x.com/artificialanlys/status/1990926803087892506" target="_blank" rel="noopener">x.com/artificialan...</a><br>report: <a href="https://artificialanalysis.ai/evaluations/omniscience" target="_blank" rel="noopener">artificialanalysis.ai/evaluations/...</a><br><br>3. Demis Hassabis said 1-2 months ago that major version numbers indicate OOM scaling, minor is RL scaling</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreid36r7p6qxouwaa7zkbvjff7ixl3ujsirdlejv7nlpd7hbaio3ega@jpeg" alt="Additionally, we found there is a high correlation between the size of open weights models and Accuracy (but not Hallucination Rate). As such, Gemini 3 Pro's very high Accuracy suggests it is a very large model." class="post-image" loading="lazy">
</div>
</div>
<a href="https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3m5woagm3xc24" class="post-quote" target="_blank" rel="noopener">
<div class="quote-header">
<img src="https://cdn.bsky.app/img/avatar/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidvgcq72e5erl4stnap6wjzas6a2wburoa7yzctwuy4vgx4vb5fsi@jpeg" alt="" class="quote-avatar">
<span class="quote-author">Tim Kellogg</span>
<span class="quote-handle">@timkellogg.me</span>
</div>
<div class="quote-text">Gemini 3 is indeed a much larger model<br><br>both pre-training (model size) & post-training (RL) scaling</div>
<div class="quote-images"><img src="https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreibxn57zjvloi6v5ber32yd6otpqa6qsaahjcvrv64ydhox6qg4efq@jpeg" alt="Oriol Vinyals &
• @OriolVinyalsML •2h
The secret behind Gemini 3?
Simple: Improving pre-training & post-training
Pre-training: Contra the popular belief that scaling is over—which we discussed in our NeurIPS '25 talk with @ilyasut and @quocleix— the team delivered a drastic jump. The delta between 2.5 and 3.0 is as big as we've ever seen. No walls in sight!
Post-training: Still a total greenfield. There's lots of room for algorithmic progress and improvement, and 3.0 hasn't been an exception, thanks to our stellar team.
Congratulations to the whole team" class="quote-image" loading="lazy"></div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>22</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">so you’ve got public statements in several directions, plus unaffiliated evidence-based confirmation</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">plus this <a href="https://bsky.app/profile/natolambert.bsky.social/post/3m2zgk4nb3p2m" target="_blank" rel="noopener">bsky.app/profile/nato...</a></div>
<a href="https://bsky.app/profile/did:plc:brkj2yocng7vtggmyujy4khq/post/3m2zgk4nb3p2m" class="post-quote" target="_blank" rel="noopener">
<div class="quote-header">
<img src="https://cdn.bsky.app/img/avatar/plain/did:plc:brkj2yocng7vtggmyujy4khq/bafkreibrwiekumktnfukcbrl2w7emcft4icv2zlt6vxnorfwmwlwlzepqm@jpeg" alt="" class="quote-avatar">
<span class="quote-author">Nathan Lambert</span>
<span class="quote-handle">@natolambert.bsky.social</span>
</div>
<div class="quote-text">Recurring frontier lab gossip:<br><br>OpenAI has best post-training/rl and has pushed it super hard on weaker pretraining.<br><br>Gemini has spectacular pretraining. Making a reasoning model was super easy for them & OpenAI folks were surprised<br><br>Anthropic? Secretive i guess.</div>

</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>5</span>
</div>
</div>
<div class="thread-post">
<div class="time-elapsed">1 hour later</div>
<div class="post-text">more: verification of Nato’s observation about labs</div>
<div class="post-images multiple">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreicm7j4x7yx4p7lvbo2xgqmvgpkg77meusi6n46agxfh7pki4qlnc4@jpeg" alt="Lisan al Gaib v @scaling01
X.com
For those who have been following me for like ~ 8 months, this is no surprise.
Google always had the base-models best suited for the reasoning paradigm as seen in my analysis of AidanBench novelty scores.
This chart basically says:
Google models have the best exploration / highest mean novelty scores." class="post-image" loading="lazy">
</div>
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidks4ok7357e3e426irg77bjxin5gywlydilbagv3m454xizxdxsu@jpeg" alt="A large, wide box-and-whisker plot showing novelty score distributions for many different AI models, sorted left-to-right by increasing mean novelty. Each model along the x-axis has:
	•	A vertical box plot in a muted color (reds, greens, blues, neutrals) depending on its family.
	•	Hundreds of scattered individual points overlaid, showing the distribution of novelty scores for that model.
	•	Model names angled diagonally along the bottom, including Meta Llama models, Mistral models, Anthropic Claude models, OpenAI models, and Google Gemini models.

On the y-axis, values range roughly from 0.0 to 1.0, labeled “Novelty (Embedding Distance/Unit)”. The box plots show medians generally around 0.2–0.35, with some families slightly higher or lower depending on model. A few models have visibly wider spreads with many high-scoring outliers above 0.6–0.8.

To the right is a tall legend listing all models, each with a colored square matching the box-plot color used in the chart for that model." class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">there’s people on here that are convinced that Gemini 3 Pro is small, but i just don’t see any reliable evidence of that<br><br>being fast just means it’s a sparse MoE, which is 100% normal these days, would be surprising if it weren’t</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>8</span>
</div>
</div>
<div class="thread-post">
<div class="time-elapsed">21 hours later</div>
<div class="post-text">update: scaling01’s initial 7.5T estimate was based on fp4, but there’s no evidence that TPUv7 actually supports fp4, so estimate is revised down to ~5T<br><br>fyi <a href="https://bsky.app/profile/did:plc:6ond5sxlegjxpe3ismrczk3r" target="_blank" rel="noopener">@harsimony.bsky.social</a></div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>1</span>
</div>
</div>