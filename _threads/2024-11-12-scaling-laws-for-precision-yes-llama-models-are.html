---
layout: thread
title: "Scaling Laws for Precision"
date: 2024-11-12 17:04:26 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3larcuuemac2n
likes: 22
reposts: 3
post_count: 2
summary: "Scaling Laws for Precision  yes, llama models are harder to quantize. They’re “overtrained”, on more data, so quantization removes a lot of critical i..."
similar:
  - url: "/threads/2025-07-06-new-3-token-attention-reduces-pre-training-data-re/"
    title: "new 3-token attention reduces pre-training data requirements"
  - url: "/threads/2025-02-03-s1-simple-inference-time-scaling-this-is-a-simpl/"
    title: "s1: Simple inference-time scaling"
  - url: "/threads/2025-11-16-physics-of-language-models-part-31-if-you-show/"
    title: "Physics of Language Models: Part 3.1"
---
<div class="thread-post">
<div class="post-text">Scaling Laws for Precision<br><br>yes, llama models are harder to quantize. They’re “overtrained”, on more data, so quantization removes a lot of critical information <br><br><a href="https://arxiv.org/abs/2411.04330" target="_blank" rel="noopener">arxiv.org/abs/2411.04330</a></div>
<a href="https://arxiv.org/abs/2411.04330" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">arxiv.org</div>
<div class="embed-title">Scaling Laws for Precision</div>
<div class="embed-description">Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise "precision-aware" scaling laws for b...</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>22</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">This whole paper is a whole lot of validation for how I assumed things work<br><br>e.g. a 1B model trained in fp4 has the same effective parameter count as a 256M model trained in bf16</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>2</span>
</div>
</div>