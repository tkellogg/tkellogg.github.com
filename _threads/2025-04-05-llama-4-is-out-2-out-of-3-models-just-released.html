---
layout: thread
title: "üö®Llama 4 Is Out!üö®"
date: 2025-04-05 19:26:36 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lm3nxnq6sc2d
likes: 52
reposts: 13
post_count: 9
summary: "üö®Llama 4 Is Out!üö®  2 out of 3 models just released  - Scout: 109B / 17B active - Maverick: 400B / 17B active - Bohemoth: 2T / 288B active  ai.meta.com..."
similar:
  - url: "/threads/2025-04-06-huge-1t-models-are-fascinating-bc-theyre-like-tr/"
    title: "huge 1T+ models are fascinating bc they‚Äôre like tree rings. they take so long..."
  - url: "/threads/2025-07-14-meta-superintelligence-msl-is-scrapping-llama-4/"
    title: "Meta Superintelligence (MSL) is scrapping Llama 4 & probably abandoning open ..."
  - url: "/threads/2025-11-24-my-benchmark-for-ai-models-is-how-much-they-change/"
    title: "my benchmark for AI models is how much they change life for me, normally it t..."
---
<div class="thread-post">
<div class="post-text">üö®Llama 4 Is Out!üö®<br><br>2 out of 3 models just released<br><br>- Scout: 109B / 17B active<br>- Maverick: 400B / 17B active<br>- Bohemoth: 2T / 288B active<br><br><a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" target="_blank" rel="noopener">ai.meta.com/blog/llama-4...</a></div>
<a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">ai.meta.com</div>
<div class="embed-title">The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation</div>
<div class="embed-description">We‚Äôre introducing Llama 4 Scout and Llama 4 Maverick, the first open-weight natively multimodal models with unprecedented context support and our first built using a mixture-of-experts (MoE) architect...</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>52</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>13</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">fascinating that they kept the active size the same between scout & maverick, this is going to be fun to dig into</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>6</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">i made that unclear, but behemoth is still in training</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>5</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">oooo, it‚Äôs early fusion!<br><br>iirc gpt4o is still not early fusion, right?</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreic3fnbpk2mh2nudcegmw2rxrbp55fhfwh4reguzhud2zqnzcydetq@jpeg" alt="Llama 4 models are designed with native multimodality, incorporating early fusion to seamlessly integrate text and vision tokens into a unified model backbone. Early fusion is a major step forward, since it enables us to jointly pre-train the model with large amounts of unlabeled text, image, and video data. We also improved the vision encoder in Llama 4. This is based on MetaCLIP but trained separately in conjunction with a frozen Llama model to better adapt the encoder to the LLM." class="post-image" loading="lazy">
<div class="image-alt">Llama 4 models are designed with native multimodality, incorporating early fusion to seamlessly integrate text and vision tokens into a unified model backbone. Early fusion is a major step forward, since it enables us to jointly pre-train the model with large amounts of unlabeled text, image, and video data. We also improved the vision encoder in Llama 4. This is based on MetaCLIP but trained separately in conjunction with a frozen Llama model to better adapt the encoder to the LLM.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>6</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">fp8 is the new hotness, thanks deepseek</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidbtffjis5vop4abmocbhnmvjnnpymse5u3w3gbbu2pwtyxzjzjqq@jpeg" alt="Additionally, we focus on efficient model training by using FP8 precision, without sacrificing quality and ensuring high model FLOPs utilization‚Äîwhile pre-training our Llama 4 Behemoth model using FP8 and 32K GPUs, we achieved 390 TFLOPs/GPU. The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets." class="post-image" loading="lazy">
<div class="image-alt">Additionally, we focus on efficient model training by using FP8 precision, without sacrificing quality and ensuring high model FLOPs utilization‚Äîwhile pre-training our Llama 4 Behemoth model using FP8 and 32K GPUs, we achieved 390 TFLOPs/GPU. The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>4</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">oh damn! scout has a 10M context width!</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreic6ujf5lh6jhrhi52vx3isbk43oukke5tutufqooccqyolte3x57y@jpeg" alt="We continued training the model in what we call ‚Äúmid-training‚Äù to improve core capabilities with new training recipes including long context extension using specialized datasets. This enabled us to enhance model quality while also unlocking best-in-class 10M input context length for Llama 4 Scout." class="post-image" loading="lazy">
<div class="image-alt">We continued training the model in what we call ‚Äúmid-training‚Äù to improve core capabilities with new training recipes including long context extension using specialized datasets. This enabled us to enhance model quality while also unlocking best-in-class 10M input context length for Llama 4 Scout.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">s1 strikes again ‚Äî difficult problems are crucial. in post training they used llama3 to curate the dataset down to just hard problems <br><br>oh btw, something from the DS paper today ‚Äî small LLMs are unreasonably good judges and critics</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreibawsdnxhuzehp6srncyndvxvxfdeej64s6om7vq5wumltoochc7m@jpeg" alt="A key learning was that SFT and DPO can over-constrain the model, restricting exploration during the online RL stage and leading to suboptimal accuracy, particularly in reasoning, coding, and math domains. To address this, we removed more than 50% of our data tagged as easy by using Llama models as a judge and did lightweight SFT on the remaining harder set. In the subsequent multimodal online RL stage, by carefully selecting harder prompts, we were able to achieve a step change in performance. " class="post-image" loading="lazy">
<div class="image-alt">A key learning was that SFT and DPO can over-constrain the model, restricting exploration during the online RL stage and leading to suboptimal accuracy, particularly in reasoning, coding, and math domains. To address this, we removed more than 50% of our data tagged as easy by using Llama models as a judge and did lightweight SFT on the remaining harder set. In the subsequent multimodal online RL stage, by carefully selecting harder prompts, we were able to achieve a step change in performance. </div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>4</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">whoah, interleaved attention layers with no positional embeddings<br><br>i‚Äôll have to dig into iRoPE</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidwetujpitmoayw4zhys2p6iwjw77rk773f5eo72qosndgx7j4iaq@jpeg" alt="A key innovation in the Llama 4 architecture is the use of interleaved attention layers without positional embeddings. Additionally, we employ inference time temperature scaling of attention to enhance length generalization. We call this the iRoPE architecture, where ‚Äúi‚Äù stands for ‚Äúinterleaved‚Äù attention layers, highlighting the long-term goal of supporting ‚Äúinfinite‚Äù context length, and ‚ÄúRoPE‚Äù refers to the rotary position embeddings employed in most layers." class="post-image" loading="lazy">
<div class="image-alt">A key innovation in the Llama 4 architecture is the use of interleaved attention layers without positional embeddings. Additionally, we employ inference time temperature scaling of attention to enhance length generalization. We call this the iRoPE architecture, where ‚Äúi‚Äù stands for ‚Äúinterleaved‚Äù attention layers, highlighting the long-term goal of supporting ‚Äúinfinite‚Äù context length, and ‚ÄúRoPE‚Äù refers to the rotary position embeddings employed in most layers.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>8</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">looks like Groq is hosting Scout & Maverick at a 4-bit quant <a href="https://console.groq.com/docs/models" target="_blank" rel="noopener">console.groq.com/docs/models</a></div>
<a href="https://console.groq.com/docs/models" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">console.groq.com</div>
<div class="embed-title">GroqCloud - Build Fast</div>
<div class="embed-description">Build Fast with GroqCloud</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>2</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>