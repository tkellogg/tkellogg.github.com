---
layout: thread
title: "HRM: Hierarchical Reasoning Model"
date: 2025-07-27 15:27:38 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3luxfcew7j22s
likes: 67
reposts: 16
post_count: 7
summary: "HRM: Hierarchical Reasoning Model  ngl this sounds like bullshit but i don’t think it is  - 27M (million parameters) - 1000 training examples - beats ..."
similar:
  - url: "/threads/2025-08-15-hrm-confirmed-by-arc-agi-team-but-also-dismissed/"
    title: "HRM confirmed by ARC-AGI team, but also dismissed as non-generalizable"
  - url: "/threads/2025-08-04-hrm-analysis-by-dorialexanderbskysocial-the-a/"
    title: "HRM analysis by @dorialexander.bsky.social "
  - url: "/threads/2025-01-26-a-researcher-on-x-explains-why-rl-alone-didnt-wor/"
    title: "a researcher on X explains why RL alone didn’t work before "
---
<div class="thread-post">
<div class="post-text">HRM: Hierarchical Reasoning Model<br><br>ngl this sounds like bullshit but i don’t think it is<br><br>- 27M (million parameters)<br>- 1000 training examples<br>- beats o3-mini on ARC-AGI<br><br><a href="https://arxiv.org/abs/2506.21734" target="_blank" rel="noopener">arxiv.org/abs/2506.21734</a></div>
<a href="https://arxiv.org/abs/2506.21734" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">arxiv.org</div>
<div class="embed-title">Hierarchical Reasoning Model</div>
<div class="embed-description">Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT...</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>67</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>16</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">here’s a good take, comparing HRM to quaternion process theory from neuroscience<br><br>it models cognition through 2 dimensions<br><br>1. fluency vs empathy<br>2. fast vs slow<br><br><a href="https://medium.com/intuitionmachine/the-hierarchical-reasoning-model-through-the-lens-of-quaternion-process-theory-thinking-fast-and-1fc948dad97f" target="_blank" rel="noopener">medium.com/intuitionmac...</a></div>
<a href="https://medium.com/intuitionmachine/the-hierarchical-reasoning-model-through-the-lens-of-quaternion-process-theory-thinking-fast-and-1fc948dad97f" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">medium.com</div>
<div class="embed-title">The Hierarchical Reasoning Model Through the Lens of Quaternion Process Theory: Thinking Fast and…</div>
<div class="embed-description">Introduction: Mapping Artificial Intelligence to Cognitive Quaternions</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>6</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">the problem as they see it: CoT reasoning isn’t compatible with the bitter lesson, it requires too much human involvement to get right</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreihe2y7cquoudo5mz2dbs3443w3dusj4dd6t2srxnolavkjl46dzky@jpeg" alt="However, CoT for reasoning is a crutch, not a satisfactory solution. It relies on brit-tle, human-defined decompositions where a single misstep or a misorder of the steps can derail the reasoning process entirely 12, 13. This dependency on explicit linguistic steps tethers reasoning to patterns at the token level. As a result, CoT reasoning often requires significant amount of training data and generates a large number of tokens for complex reasoning tasks, resulting in slow response times. A more efficient approach is needed to minimize these data requirements 14" class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>11</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">what’s notable about this paper is how often they refer back to biology <br><br>imo that’s a sign of a big breakthrough — combining domains and getting real results</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreicqabjp3tjt42k5xz2ahnvra2kecf5whcttfkb6tdiyghgnepig5e@jpeg" alt="The human brain provides a compelling blueprint for achieving the effective computational depth that contemporary artificial models lack. It organizes computation hierarchically across cortical regions operating at different timescales, enabling deep, multi-stage reasoning 20, 21, 22. Recurrent feedback loops iteratively refine internal represen-tations, allowing slow, higher-level areas to guide, and fast, lower-level circuits to execute-subordi-nate processing while preserving global coher-
23
ence
24 25
Notably, the brain achieves such
depth without incurring the prohibitive credit-as-signment costs that typically hamper recurrent networks from backpropagation through time
19 26
**********••" class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>9</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">HRM is four learnable modules<br><br>1. input network<br>2. recurrent low-level<br>3. recurrent high-level<br>4. output network<br><br>the low-level module executes several times for each high-level iteration (i.e. there’s more compute units/neurons in the high-level module)</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>10</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">i’d love to see more. the architecture definitely has limitations. but the model is *tiny* and appears to be quite adaptive</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>9</span>
</div>
</div>
<div class="thread-post">
<div class="time-elapsed">9 hours later</div>
<div class="post-text"><a href="https://bsky.app/profile/timkellogg.me/post/3luye6ot4cs2x" target="_blank" rel="noopener">bsky.app/profile/timk...</a></div>
<a href="https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3luye6ot4cs2x" class="post-quote" target="_blank" rel="noopener">
<div class="quote-header">
<img src="https://cdn.bsky.app/img/avatar/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidvgcq72e5erl4stnap6wjzas6a2wburoa7yzctwuy4vgx4vb5fsi@jpeg" alt="" class="quote-avatar">
<span class="quote-author">Tim Kellogg</span>
<span class="quote-handle">@timkellogg.me</span>
</div>
<div class="quote-text">Opus made me this to explain it<br><br>it's a lot more complex than a simple RNN. Basically a "figure 8", where each loop is feeding into the other<br><br>* H-module grounds L-mod to keep it on track<br>* L-mod reports findings to H-module to plan next strategic step.</div>

</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>4</span>
</div>
</div>