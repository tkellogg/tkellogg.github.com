---
layout: thread
title: "notable: they ripped out the silicon that supports training"
date: 2025-11-07 00:43:55 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3m4ytwzin3k2r
likes: 23
reposts: 2
post_count: 2
summary: "notable: they ripped out the silicon that supports training  they say: “it’s the age of inference”  which, yeah, RL is mostly inference. Continual lea..."
similar:
  - url: "/threads/2025-10-31-astonishing-using-fp16-instead-of-bf16-results-in/"
    title: "astonishing: using fp16 instead of bf16 results in more stable training runs ..."
  - url: "/threads/2025-06-17-my-dumbass-version-of-this-thread-minimax-m1-dis/"
    title: "my dumbass version of this thread:"
  - url: "/threads/2025-07-08-smollm3-a-highly-detailed-look-into-modern-model/"
    title: "SmolLM3: a highly detailed look into modern model training"
---
<div class="thread-post">
<div class="post-text">notable: they ripped out the silicon that supports training<br><br>they say: “it’s the age of inference”<br><br>which, yeah, RL is mostly inference. Continual learning is almost all inference. Ambient agents, fast growing inference demands in general audiences<br><br><a href="https://kartik343.wixstudio.com/blogorithm/post/google-ironwood-a-developer-s-inside-look-at-the-first-inference-optimized-tpu" target="_blank" rel="noopener">kartik343.wixstudio.com/blogorithm/p...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreiczbdktzpxhd3opo7awwfubfesbtstxaqksz7serhomfp3irkjilu@jpeg" alt="Key Architecture Innovations
Ironwood's matrix multiply units (MXUs) have been entirely reengineered for inference-only operations with some major differences from training-centric architectures:" class="post-image" loading="lazy">
</div>
</div>
<a href="https://bsky.app/profile/did:plc:cq4gg3odxz2pzmkx2fuac3u3/post/3m4ybeuf6ac2t" class="post-quote" target="_blank" rel="noopener">
<div class="quote-header">
<img src="https://cdn.bsky.app/img/avatar/plain/did:plc:cq4gg3odxz2pzmkx2fuac3u3/bafkreihoau37nugj5kv6mhdywghrqxd7ge6zhljihicdrvgyiykrupr7rq@jpeg" alt="" class="quote-avatar">
<span class="quote-author">Sung Kim</span>
<span class="quote-handle">@sungkim.bsky.social</span>
</div>
<div class="quote-text">Google's 7th gen TPU Ironwood<br><br>10X peak performance improvement vs. TPU v5p, and more than 4X better performance per chip for both training + inference workloads vs. TPU v6e (Trillium). <br><br>They used TPUs to train + serve our own frontier models, including Gemini 3?<br><br>cloud.google.com/blog/product...</div>

</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>23</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>2</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">removing MXUs lets them reallocate transistor budget to inference tasks<br><br>they also dropped fp64 support — they’re all in on AI, it’s not going to be very useful for traditional supercomputing workloads</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>6</span>
</div>
</div>