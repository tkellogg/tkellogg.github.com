---
layout: thread
title: "Longcat-Flash-Chat (560B)"
date: 2025-08-31 11:20:08 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lxoxs2ddyk2c
likes: 45
reposts: 5
post_count: 6
summary: "Longcat-Flash-Chat (560B)  uh, holy shit this one is intriguing. bare minimum they compare themselves to all the (actual) top models and do okay   but..."
similar:
  - url: "/threads/2025-07-13-fascinating-blog-by-a-developer-on-k2-they-talk-a/"
    title: "fascinating blog by a developer on K2"
  - url: "/threads/2025-09-25-chatgpt-pulse-say-some-vague-wish-during-chat-an/"
    title: "ChatGPT Pulse"
  - url: "/threads/2025-10-21-zai-released-a-paper-very-similar-to-deepseek-ocr/"
    title: "Z.ai released a paper very similar to DeepSeek-OCR on the same exact day (a f..."
---
<div class="thread-post">
<div class="post-text">Longcat-Flash-Chat (560B)<br><br>uh, holy shit this one is intriguing. bare minimum they compare themselves to all the (actual) top models and do okay <br><br>but inside.. damn this one has some cool ideas<br><br><a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Chat" target="_blank" rel="noopener">huggingface.co/meituan-long...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidsx7m6uajwgky6dq3sd7zwz4vf4adkv4uz7yqq6qnubqgjrblkti@jpeg" alt="The image is a multi-panel bar chart comparing performance of different large language models across several benchmarks. It is divided into four categories: General Domains, Agentic Tool Use, Code, and Instruction Following. Each panel has bars representing model results, with scores on the y-axis.

Top row â€“ General Domains:
	â€¢	ArenaHard-V2: LongGPT-Flash leads with 86.5, followed by Kimi K2 (88.2), DeepSeek V3.1 (84.1), Claude Sonnet (61.5), GPT-4.1 (62.1), Qwen3.5 MoE-2507 (85.7), and Gemini 2.5 Flash (77.0).
	â€¢	MMLU-Pro: Best scores are Kimi K2 (84.5) and DeepSeek V3.1 (84.5), with LongGPT-Flash (82.7), Qwen3.5 MoE-2507 (82.1), GPT-4.1 (81.7), Claude Sonnet (83.7), Gemini 2.5 Flash (82.0).

Top row â€“ Agentic Tool Use:
	â€¢	t2-Bench (average): LongGPT-Flash leads (67.7), Kimi K2 (64.2), Claude Sonnet (62.1), GPT-4.1 (55.1), DeepSeek V3.1 (49.8), Qwen3.5 MoE-2507 (43.0), Gemini 2.5 Flash (40.9).
	â€¢	VitaBench: LongGPT-Flash 24.3, Claude Sonnet 23.0, DeepSeek V3.1 20.3, Kimi K2 18.2, GPT-4.1 19.0, Qwen3.5 MoE-2507 8.5, Gemini 2.5 Flash 8.0.

Bottom row â€“ Code:
	â€¢	SWE-Bench-Verified: Claude Sonnet leads with 68.0, Kimi K2 64.6, DeepSeek V3.1 66.0, LongGPT-Flash 60.4, GPT-4.1 48.6, Qwen3.5 MoE-2507 42.0, Gemini 2.5 Flash 40.6.
	â€¢	TerminalBench: Claude Sonnet 40.7, LongGPT-Flash 39.5, DeepSeek V3.1 31.3, GPT-4.1 28.4, Kimi K2 25.9, Qwen3.5 MoE-2507 17.3, Gemini 2.5 Flash 12.4.

Bottom row â€“ Instruction Following:
	â€¢	COLLIE: LongGPT-Flash 57.1, Kimi K2 56.3, Claude Sonnet 51.2, GPT-4.1 50.0, DeepSeek V3.1 49.7, Gemini 2.5 Flash 48.6, Qwen3.5 MoE-2507 43.8.
	â€¢	Meeseeks (ZH): LongGPT-Flash 43.0, Kimi K2 42.8, Claude Sonnet 41.5, GPT-4.1 35.1, DeepSeek V3.1 35.3, Qwen3.5 MoE-2507 33.8, Gemini 2.5 Flash 34.8.
" class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>45</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>5</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">most interesting â€” dynamic computation<br><br>not only is it fairly sparse MoE, each token can receive dynamically more compute via a PID controller for bias adjustment ðŸ¤¯ <br><br>so when it gets to a token that requires extra thought, itâ€™ll just spin there, computing more</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreic6u7mk7anngnqgtuu2tap5woo36bahyj7jnu3l6qejhpiuhdaeme@jpeg" alt="LongCat-Flash is designed and optimized under two key principles: efficient computation utilization, as well as efficient training and inference. Specifically, (1) As not all tokens are equal, we introduce the zero-computation experts mechanism in MoE blocks to allocate a dynamic computation budget to important tokens based on their significance, i.e., activating 18.6 to 31.3 billion parameters (out of 560 billion total) based on contextual demands. To ensure consistent computation load, we employ expert bias adjusted by a PID-controller, maintaining an average of~27 billion activated parameters per token. (2) As communication overhead becomes a bottleneck
during MoE model scaling, we incorporate the Shortcut-connected MoE (ScMoE) design to expand
the computation-communication overlap window.
Combined with customized infrastructure
optimizations, this design enables training at a massive scale of over tens of thousands accelerators and inference with high throughput
and low latency." class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>14</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">how are chinese labs cutting their dependence on NVIDIA? like this:<br><br>run experiments on tiny models, transfer hyperparameters (result of experiments) to a far larger model for the yolo run<br><br><a href="https://bsky.app/profile/timkellogg.me/post/3lxmq26ikks2t" target="_blank" rel="noopener">bsky.app/profile/timk...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreie6nyyllk7627dmpldktrvnviy6du2xg76pntl26y5gsbmgvezzoq@jpeg" alt="models: (1) We successfully apply a hyperparameter transfer strategy to such a large model, predicting optimal hyperparameter configurations by leveraging results from smaller proxy models with theoretical guarantees. (2) We" class="post-image" loading="lazy">
</div>
</div>
<a href="https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lxmq26ikks2t" class="post-quote" target="_blank" rel="noopener">
<div class="quote-header">
<img src="https://cdn.bsky.app/img/avatar/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidvgcq72e5erl4stnap6wjzas6a2wburoa7yzctwuy4vgx4vb5fsi@jpeg" alt="" class="quote-avatar">
<span class="quote-author">Tim Kellogg</span>
<span class="quote-handle">@timkellogg.me</span>
</div>
<div class="quote-text">DeepSeek is reducing their dependence on NVIDIA<br><br>they do small scale training runs & experiments on Huawei Ascend, but yolo runs on NVIDIA</div>
<div class="quote-images"><img src="https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreihgo7ejhkoydtnkwtr4i4q6kulqyxc7klezw2i3ywxemgmyw7qbrq@jpeg" alt="" class="quote-image" loading="lazy"></div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>16</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>2</span>
</div>
</div>
<div class="thread-post">
<div class="time-elapsed">1 hour later</div>
<div class="post-text">oh this took me too long to figure out â€” the "zero computation experts"<br><br>they have a (mostly) regular MoE router, but some of the experts are actually nothing at all. So the MoE router sometimes entirely skips experts</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>13</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">the "shortcut-connected MoE" part is solving a more complex problem than it seems on the surface<br><br>the problem is the hand-off between attention & MoE causes communication overhead (e.g. expert is located on a different GPU)<br><br>ScMoE re-orders the pipeline, better utilizing compute</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>10</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">i feel like this is the sort of shit you see when the US Government locks down compute bandwidth but not compute itself. We saw something similar with DeepSeek slinging their own PTX instead of CUDA to get around the nerfed comms</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>13</span>
</div>
</div>