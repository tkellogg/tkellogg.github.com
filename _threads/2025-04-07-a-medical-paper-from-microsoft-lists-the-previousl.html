---
layout: thread
title: "A medical paper from Microsoft lists the previously unknown model sizes of po..."
date: 2025-04-07 19:53:13 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lmaqf3qfhs25
likes: 43
reposts: 9
post_count: 2
summary: "A medical paper from Microsoft lists the previously unknown model sizes of popular closed LLMs  - Sonnet3.5: ~175B - GPT3.5-turbo: 175B - GPT4: 1.76T ..."
similar:
  - url: "/threads/2025-08-05-to-think-that-o3-mini-was-my-choice-model-for-a-lo/"
    title: "to think that o3-mini was my choice model for a long time, and now gpt-oss:20..."
  - url: "/threads/2025-08-05-gpt-oss-openais-open-weights-model-120b-20b-v/"
    title: "gpt-oss, OpenAI's open weights model"
  - url: "/threads/2025-04-14-terrible-naming-shouldve-called-it-gpt-4o-final/"
    title: "terrible naming, should’ve called it gpt-4o-final(2)-large-lite"
---
<div class="thread-post">
<div class="post-text">A medical paper from Microsoft lists the previously unknown model sizes of popular closed LLMs<br><br>- Sonnet3.5: ~175B<br>- GPT3.5-turbo: 175B<br>- GPT4: 1.76T<br>- GPT4o: 200B<br>- GPT4o-mini: 8B<br>- o1-mini: 100B<br>- o1-preview: 300B<br><br><a href="https://arxiv.org/abs/2412.19260" target="_blank" rel="noopener">arxiv.org/abs/2412.19260</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreicm2eswkx3hwkt3fdrpd4yggcyt6ceu7bdrcuegc5gubwmvqraaxq@jpeg" alt="This image shows section 5.1 Language Models from a document’s “Experiments & Results” chapter. It details the language models used in the study:
	1.	Phi-3-7B: A Small Language Model (SLM) with 7 billion parameters [Abdin et al., 2024].
	2.	Claude 3.5 Sonnet (dated 2024-10-22): A ~175B parameter model offering state-of-the-art performance in coding, vision, and reasoning [Anthropic, 2024].
	3.	Gemini 2.0 Flash: Described as Google’s most advanced Gemini model [Google, 2024], with mentions of other large models like Med-PaLM (540B) [Singhal et al., 2023].
	4.	ChatGPT (~175B) and GPT-4 (~1.76T): Called “high-intelligence” models [OpenAI, 2023a & 2023b].
	5.	GPT-4o (~200B): Promoted as “GPT-4-level intelligence but faster” [OpenAI, 2024a], and includes GPT-4o-mini (~8B, dated 2024-05-13) [OpenAI, 2024b].
	6.	o1-mini (~100B, dated 2024-09-12) [OpenAI, 2024c], and o1-preview (~300B, same date) [OpenAI, 2024d], both described as having “new AI capabilities” for complex reasoning.

A note at the bottom explains that parameter sizes are estimates from public articles, not officially confirmed, and provided only for context. It advises readers to consult original documentation for accurate details. Also mentioned: Phi-3 and Claude required minimal post-processing for formatting fixes." class="post-image" loading="lazy">
<div class="image-alt">This image shows section 5.1 Language Models from a document’s “Experiments & Results” chapter. It details the language models used in the study:
	1.	Phi-3-7B: A Small Language Model (SLM) with 7 billion parameters [Abdin et al., 2024].
	2.	Claude 3.5 Sonnet (dated 2024-10-22): A ~175B parameter model offering state-of-the-art performance in coding, vision, and reasoning [Anthropic, 2024].
	3.	Gemini 2.0 Flash: Described as Google’s most advanced Gemini model [Google, 2024], with mentions of other large models like Med-PaLM (540B) [Singhal et al., 2023].
	4.	ChatGPT (~175B) and GPT-4 (~1.76T): Called “high-intelligence” models [OpenAI, 2023a & 2023b].
	5.	GPT-4o (~200B): Promoted as “GPT-4-level intelligence but faster” [OpenAI, 2024a], and includes GPT-4o-mini (~8B, dated 2024-05-13) [OpenAI, 2024b].
	6.	o1-mini (~100B, dated 2024-09-12) [OpenAI, 2024c], and o1-preview (~300B, same date) [OpenAI, 2024d], both described as having “new AI capabilities” for complex reasoning.

A note at the bottom explains that parameter sizes are estimates from public articles, not officially confirmed, and provided only for context. It advises readers to consult original documentation for accurate details. Also mentioned: Phi-3 and Claude required minimal post-processing for formatting fixes.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>43</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>9</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">seems like an incidental information leak. The writers clearly thought this information was public, it definitely is not.</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>6</span>
</div>
</div>