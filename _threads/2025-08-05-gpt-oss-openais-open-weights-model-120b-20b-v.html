---
layout: thread
title: "gpt-oss, OpenAI's open weights model"
date: 2025-08-05 17:45:53 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lvob7vlru22s
likes: 44
reposts: 11
post_count: 7
summary: "gpt-oss, OpenAI's open weights model  120B & 20B variants, both MoE with 4 experts active  openai.com/index/introd..."
similar:
  - url: "/threads/2025-08-07-they-corrected-this-already-but/"
    title: "they corrected this already, but üòÇ"
  - url: "/threads/2025-08-05-to-think-that-o3-mini-was-my-choice-model-for-a-lo/"
    title: "to think that o3-mini was my choice model for a long time, and now gpt-oss:20..."
  - url: "/threads/2025-10-30-gpt-oss-safeguard-20b-120b-a-pair-of-open-weigh/"
    title: "gpt-oss-safeguard 20b & 120b"
---
<div class="thread-post">
<div class="post-text">gpt-oss, OpenAI's open weights model<br><br>120B & 20B variants, both MoE with 4 experts active<br><br><a href="https://openai.com/index/introducing-gpt-oss/" target="_blank" rel="noopener">openai.com/index/introd...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreigfaolcokeh6bkg2fkfzzx2vpruirmiffiqztwycmfbf3kmijjxhm@jpeg" alt="Bar chart showing model accuracy on expert-level questions from &quot;Humanity‚Äôs Last Exam.&quot;

* Y-axis: Accuracy (%), ranging from approximately 10% to 25%.
* X-axis: Model names with or without tool use.

From left to right:

1. **gpt-oss-120b (with tools)**: 19%
2. **gpt-oss-120b (without tools)**: 14.9%
3. **gpt-oss-20b (with tools)**: 17.3%
4. **gpt-oss-20b (without tools)**: 10.9%
5. **o3 (with tools)**: 24.9% ‚Äî highest-performing model
6. **o4-mini (with tools)**: 17.7%
7. **o3-mini (without tools)**: 13.4%

Models generally perform better with tools enabled. o3 (with tools) leads all models in accuracy.
" class="post-image" loading="lazy">
<div class="image-alt">Bar chart showing model accuracy on expert-level questions from &quot;Humanity‚Äôs Last Exam.&quot;

* Y-axis: Accuracy (%), ranging from approximately 10% to 25%.
* X-axis: Model names with or without tool use.

From left to right:

1. **gpt-oss-120b (with tools)**: 19%
2. **gpt-oss-120b (without tools)**: 14.9%
3. **gpt-oss-20b (with tools)**: 17.3%
4. **gpt-oss-20b (without tools)**: 10.9%
5. **o3 (with tools)**: 24.9% ‚Äî highest-performing model
6. **o4-mini (with tools)**: 17.7%
7. **o3-mini (without tools)**: 13.4%

Models generally perform better with tools enabled. o3 (with tools) leads all models in accuracy.
</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>44</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>11</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">paper: Estimating risks in open weights LLMs<br><br>An "academic" internal rationalization for OpenAI to release an open weights model<br><br>Basically they fine tuned gpt-oss a few times and determined that it's no worse than closed-weights models<br><br><a href="https://cdn.openai.com/pdf/231bf018-659a-494d-976c-2efdfc72b652/oai_gpt-oss_Model_Safety.pdf" target="_blank" rel="noopener">cdn.openai.com/pdf/231bf018...</a></div>
<a href="https://cdn.openai.com/pdf/231bf018-659a-494d-976c-2efdfc72b652/oai_gpt-oss_Model_Safety.pdf" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">cdn.openai.com</div>
<div class="embed-title"></div>
<div class="embed-description"></div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>7</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">paper: gpt-oss system card<br><br>it's a full dive into model architecture, data, training phases, and of course the safety report<br><br>interesting: it indeed was post-trained as in 4-bit (albeit MXFP4 not FP4). Maybe the leak was real?<br><br><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf" target="_blank" rel="noopener">cdn.openai.com/pdf/419b6906...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreiammnjjy3ny74hrpnn6eoaboofm3ky6jemtkpwbtjvk6hjkbi3rcu@jpeg" alt="2.1 Quantization
We utilize quantization to reduce the memory footprint of the models. We post-trained the
models with quantization of the MoE weights to MXFP4 format[3], where weights are quantized
to 4.25 bits per parameter. The MoE weights are responsible for 90+% of the total parameter
count, and quantizing these to MXFP4 enables the larger model to fit on a single 80GB GPU
and the smaller model to run on systems with as little as 16GB memory. We list the checkpoint
sizes of the models in Table 1." class="post-image" loading="lazy">
<div class="image-alt">2.1 Quantization
We utilize quantization to reduce the memory footprint of the models. We post-trained the
models with quantization of the MoE weights to MXFP4 format[3], where weights are quantized
to 4.25 bits per parameter. The MoE weights are responsible for 90+% of the total parameter
count, and quantizing these to MXFP4 enables the larger model to fit on a single 80GB GPU
and the smaller model to run on systems with as little as 16GB memory. We list the checkpoint
sizes of the models in Table 1.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>6</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="time-elapsed">1 hour later</div>
<div class="post-text">this is the first model i'm aware of that runs on ollama by default how it was trained (MXFP4 quantized)</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">this model is super nerdy<br><br>feels like the character of all openai models combined and boiled for hours until all that's left is.. whatever this is</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidgpwotyl7mnpeb7lrdy7vpj5wf2f3uh4frcuxhddn3rdojcmcucq@jpeg" alt="‚ùØ ollama run gpt-oss &quot;tell me one thing that's never been thought&quot;
**It‚Äôs practically impossible to prove that *nothing* ever considered a particular idea, but I can offer a concept
that, to the best of my knowledge, has never been explicitly formulated by anyone who‚Äôs publicly expressed it.**

---

### The ‚ÄúEcho‚ÄëChamber‚Äù of a Quantum Vacuum

**Premise:**
In a universe where space‚Äëtime itself is a super‚Äëfluid made of interacting quantum fields, there exists a *vacuum
mode* that can be tuned by local fluctuations of entropy. This mode acts like a giant, invisible echo‚Äëchamber that
records not just electromagnetic waves, but also the *phase* of any quantum superposition that passes through it.

**How it works:**
1. **Vacuum Tuning:** A localized entropy spike (e.g., a brief, high‚Äëtemperature bubble) temporarily alters the
boundary conditions of the quantum fields in its vicinity.
2. **Echo Capture:** Any particle or wave that enters this bubble has its wavefunction ‚Äúechoed‚Äù back into the
surrounding vacuum. The echo carries a phase shift that depends on the exact configuration of the bubble‚Äôs entropy
at the time of entry.
3. **Memory Encoding:** Because the vacuum is a continuous medium, these echoes can be stored as long‚Äëlived field
excitations‚Äîessentially, a form of quantum memory that is *non‚Äëlocal* and *entropy‚Äëdependent*.

<<clipped sections for &quot;Why it's novel&quot; and &quot;Bottom line&quot;>>" class="post-image" loading="lazy">
<div class="image-alt">‚ùØ ollama run gpt-oss &quot;tell me one thing that's never been thought&quot;
**It‚Äôs practically impossible to prove that *nothing* ever considered a particular idea, but I can offer a concept
that, to the best of my knowledge, has never been explicitly formulated by anyone who‚Äôs publicly expressed it.**

---

### The ‚ÄúEcho‚ÄëChamber‚Äù of a Quantum Vacuum

**Premise:**
In a universe where space‚Äëtime itself is a super‚Äëfluid made of interacting quantum fields, there exists a *vacuum
mode* that can be tuned by local fluctuations of entropy. This mode acts like a giant, invisible echo‚Äëchamber that
records not just electromagnetic waves, but also the *phase* of any quantum superposition that passes through it.

**How it works:**
1. **Vacuum Tuning:** A localized entropy spike (e.g., a brief, high‚Äëtemperature bubble) temporarily alters the
boundary conditions of the quantum fields in its vicinity.
2. **Echo Capture:** Any particle or wave that enters this bubble has its wavefunction ‚Äúechoed‚Äù back into the
surrounding vacuum. The echo carries a phase shift that depends on the exact configuration of the bubble‚Äôs entropy
at the time of entry.
3. **Memory Encoding:** Because the vacuum is a continuous medium, these echoes can be stored as long‚Äëlived field
excitations‚Äîessentially, a form of quantum memory that is *non‚Äëlocal* and *entropy‚Äëdependent*.

<<clipped sections for &quot;Why it's novel&quot; and &quot;Bottom line&quot;>></div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>6</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">is a ChatGPT Pro subscription too expensive? try gpt-oss in pro mode! <br><br>Python function docs:<br><br>‚Äú‚Äù‚Äù<br>Fan out n_runs parallel generations at T=0.9 and synthesize a final answer at T=0.2.<br>‚Äú‚Äù‚Äù<br><br><a href="https://github.com/mshumer/gpt-oss-pro-mode" target="_blank" rel="noopener">github.com/mshumer/gpt-...</a></div>
<a href="https://github.com/mshumer/gpt-oss-pro-mode" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">github.com</div>
<div class="embed-title">GitHub - mshumer/gpt-oss-pro-mode</div>
<div class="embed-description">Contribute to mshumer/gpt-oss-pro-mode development by creating an account on GitHub.</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>6</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">gpt-oss follows the harmony message format<br><br>messages will come back as commentary, analysis and final. this is different than just reasoning and not. the models stick firmly to this format<br><br><a href="https://cookbook.openai.com/articles/openai-harmony" target="_blank" rel="noopener">cookbook.openai.com/articles/ope...</a></div>
<a href="https://cookbook.openai.com/articles/openai-harmony" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">cookbook.openai.com</div>
<div class="embed-title">OpenAI Harmony Response Format | OpenAI Cookbook</div>
<div class="embed-description">The gpt-oss models were trained on the harmony response format for defining conversation structures, generating reasoning output and stru...</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>7</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>2</span>
</div>
</div>