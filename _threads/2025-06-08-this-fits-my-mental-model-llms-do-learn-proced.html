---
layout: thread
title: "this fits my mental model — LLMs *do* learn procedures. But it’s the same mec..."
date: 2025-06-08 15:36:01 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lr46wbgwek2a
likes: 22
reposts: 3
post_count: 7
summary: "this fits my mental model — LLMs *do* learn procedures. But it’s the same mechanics as what’s learning facts. So of course it would also hallucinate p..."
similar:
  - url: "/threads/2025-09-06-hallucinations-are-accidentally-created-by-evals/"
    title: "Hallucinations are accidentally created by evals"
  - url: "/threads/2025-11-02-llms-can-report-their-own-experience-the-most-con/"
    title: "LLMs can report their own experience"
  - url: "/threads/2025-08-21-im-coming-to-the-conclusion-that-most-programmers/"
    title: "i’m coming to the conclusion that most programmers are very bad at using LLMs..."
---
<div class="thread-post">
<div class="post-text">this fits my mental model — LLMs *do* learn procedures. But it’s the same mechanics as what’s learning facts. So of course it would also hallucinate procedures<br><br>but also: what does procedure hallucination look like? i don’t think i have a grasp on that <br><br><a href="https://machinelearning.apple.com/research/illusion-of-thinking" target="_blank" rel="noopener">machinelearning.apple.com/research/ill...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreie7jmwn7vvkm5juwxo5d2kzvujhnm44gmah5d7dkskb7emt6e7zhy@jpeg" alt="By comparing LRMs with their standard LLM counterparts under equivalent inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity tasks where additional thinking in LRMs demons stes advantage, and (3) high-complexity tasks
where both models experience complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across puzzles. We also investigate the reasoning traces in more depth, studying the patterns of explored soli
ons
and analyzing the models' computational behavior, shedding light on their strengths, limitations, and ultimately raising crucial questions about their true reasoning capabilities." class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>22</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">This paper is being advertised as evidence against AGI, but I’m looking at these charts and…that’s how people operate too. That’s humans’ failure modes. Computers’ normally look entirely different. This is AGI straight into your veins</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreicr4zssgqbasim7yh2dnisnwqevbzlaao46zn7bbotux2km5ytg3i@jpeg" alt="This image presents experimental results on how Claude 3.7 and Claude 3.7 (+thinking) solve Tower of Hanoi puzzles at increasing complexity levels.

Top row of plots:
	•	Left plot (Accuracy vs. Complexity): Claude 3.7 (+thinking) maintains 100% accuracy through 6 disks and then declines sharply. Claude 3.7 without thinking fails beyond 4 disks.
	•	Middle plot (Token usage vs. Complexity): The thinking model uses more tokens, peaking at 20,000 for 10 disks, whereas non-thinking Claude uses fewer tokens overall but also collapses early.
	•	Right plot (Position within thoughts): Shows when final answers occur in the trace. Correct solutions appear early for easy tasks, and later for harder ones. Incorrect ones often show premature termination.

Figure caption highlights:
	•	Bottom left & middle: Non-thinking models are more accurate and efficient at low complexity. But as task difficulty increases, thinking models outperform—though at the cost of more tokens—until a failure threshold is hit.
	•	Bottom right: In successful attempts, Claude 3.7 Thinking gives correct answers either early (simple cases) or late (harder cases). In failed attempts, it latches onto incorrect answers too early and wastes remaining tokens. This exposes inefficiencies in its reasoning approach." class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>6</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">this is a fine conclusion, but it’s phrased poorly<br><br>it leads the reader to believe that LRMs are inherently limited, whereas it’s actually just saying that LRMs aren’t computers, they’re something else<br><br>again, that’s what we’ve been saying</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreicqccaqvys5rqheuls3wy6q77dnhpekaii5vtmejjei6bynphsgqa@jpeg" alt="For simpler, low-compositional problems, standard LLMs demonstrate greater efficiency and accuracy. As problem complexity moderately increases, thinking models gain an advantage. However, when problems reach high complexity with longer compositional depth, both model types experience complete performance collapse (Fig. 1, bottom left). Notably, near this collapse point, LRMs begin reducing their reasoning effort (measured by inference-time tokens) as problem com 'exity increases, despite operating well below generation length limits (Fig. 1 bottom middle). This suggests a fundamental inference time scaling limitation in LRMs' reasoning capabilities relative to problem complexity
Finally, our analysis of intermediate reasoning traces or
thoughts reveals complexity-dependent pat.rns: In simpler problems, reasoning models often identify correct solutions early but inefficiently continue exploring incorrect alternatives an &quot;overthinking&quot; phenomenon. At moderate complexity, correct solutions emerge only after extensive exploration of incorrect paths. Beyond a certain complexity threshold, models completely fail to find correct solutions (Fig. I, bottom right). This indicates LRMs possess limited self-correction capabilities that, while valuable, reveal fundamental inefficiencies and clear scaling limitations." class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>2</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">all of this is very intuitive <br><br>some models are inherently more capable than others. Simply “thinking longer” doesn’t magically solve harder problems. The LLM still must be capable of tackling said problem <br><br>everyone i’ve ever talked to has this intuition, technical or not</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>4</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">the labs have also been saying this<br><br>OpenAI and Anthropic have been talking about how making this new crop of models is a combo of pre & post training scaling<br><br>if they still need pre-training, that means they still need to improve the fundamental nature of the model. Thinking isn’t a panacea</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">there was a time where i did think that reasoning would be a panacea. simply thinking longer during inference would tackle all problems <br><br>but that phase didn’t last long. even by the time my s1 post landed it didn’t feel right<br><br><a href="https://timkellogg.me/blog/2025/02/03/s1" target="_blank" rel="noopener">timkellogg.me/blog/2025/02...</a></div>
<a href="https://timkellogg.me/blog/2025/02/03/s1" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">timkellogg.me</div>
<div class="embed-title">S1: The $6 R1 Competitor?</div>
<div class="embed-description"></div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>5</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">for example, if you misconfigure ollama so that it forgets to stop on the stop token, the response doesn’t get better, it gets FAR worse<br><br>so then maybe you can RL it into just thinking longer<br><br>s1 showed that was VERY easy (force it to say “wait,”) but we didn’t see anything like takeoff</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>3</span>
</div>
</div>