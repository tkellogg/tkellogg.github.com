---
layout: thread
title: "Gemini 3 model card leaked"
date: 2025-11-18 12:22:44 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3m5vq4qb6zc2y
likes: 65
reposts: 9
post_count: 7
summary: "Gemini 3 model card leaked  the URL is taken down now, was here:    storage.googleapis.com/deepmind-med..."
similar:
  - url: "/threads/2025-11-13-gemini-3-on-par-with-experts-this-riveting-surpr/"
    title: "Gemini 3 on par with experts"
  - url: "/threads/2025-06-17-gemini-25-tech-report-is-out-the-tech-report-go/"
    title: "Gemini 2.5 tech report is out!"
  - url: "/threads/2025-11-19-evidence-that-gemini-3-is-very-large-1-the-qt-2/"
    title: "Evidence that Gemini 3 is very large:"
---
<div class="thread-post">
<div class="post-text">Gemini 3 model card leaked<br><br>the URL is taken down now, was here:  <br><br><a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" target="_blank" rel="noopener">storage.googleapis.com/deepmind-med...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreia3xschamtgnhbkitembtnl5oa7rovbz2oixtdiiij5hbw6m5yogq@jpeg" alt="
Below is a faithful transcription of all visible entries:

⸻

Benchmark — Description — Scores

Humanity’s Last Exam — Academic reasoning, no tools
	•	Gemini 3 Pro 37.5%
	•	Gemini 2.5 Pro 21.6%
	•	Claude Sonnet 4.5 13.7%
	•	GPT-5.1 26.5%

ARC-AGI-2 — Visual reasoning puzzles (ARC Prize Verified)
	•	31.1% — 4.9% — 13.6% — 17.6%

GPOA Diamond — Scientific knowledge, no tools
	•	91.9% — 86.4% — 83.4% — 88.1%

AIME 2025 — Mathematics, no tools
	•	95.0% — 88.0% — 87.0% — 94.0%
	•	A second line shows: 100% — — 100% — —

MathArena Apex — Challenging Math Contest problems
	•	23.4% — 0.5% — 1.6% — 1.0%

MMMU-Pro — Multimodal understanding and reasoning
	•	81.0% — 68.0% — 68.0% — 80.8%

ScreenSpot-Pro — Screen understanding
	•	72.7% — 11.4% — 36.2% — 3.5%

CharXiv Reasoning — Information synthesis from complex charts
	•	81.4% — 69.6% — 68.5% — 69.5%

OmniDocBench 1.5 — OCR (lower is better: Overall Edit Distance)
	•	0.115 — 0.147 — 0.147 — 0.147

Video-MMMU — Knowledge acquisition from videos
	•	87.6% — 83.6% — 77.8% — 80.4%

LiveCodeBench Pro — Competitive coding (Elo rating, higher is better)
	•	2,439 — 1,775 — 1,418 — 2,243

Terminal-Bench 2.0 — Agentic coding (Terminus-2 agent)
	•	54.2% — 32.6% — 42.8% — 47.6%

SWE-Bench Verified — Agentic coding (single attempt)
	•	76.2% — 59.6% — 77.2% — 76.3%

t2-bench — Agentic tool use
	•	85.4% — 54.9% — 84.7% — 80.2%

Vending-Bench 2 — Long-horizon agentic tasks (Net worth, higher is better)
	•	$5,478.16 — $573.64 — $3,838.74 — $1,473.43

FACTS Benchmark Suite — Internal grounding, parametric knowledge, search retrieval
	•	70.5% — 63.4% — 50.4% — 50.8%

SimpleQA Verified — Parametric knowledge
	•	72.1% — 54.5% — 29.3% — 34.9%

MMLU — Multilingual Q&A
	•	91.8% — 89.5% — 89.1% — 91.0%

Global PIQA — Commonsense reasoning across 100+ languages
	•	93.4% — 91.5% — 90.1% — 90.9%

MRCR v2 (8-needle) — Long-context performance
	•	77.0% — 58.0% — 47.1% — 61.6%
	•	Second line: 26.3% — 16.4% — not supported — not supported" class="post-image" loading="lazy">
<div class="image-alt">
Below is a faithful transcription of all visible entries:

⸻

Benchmark — Description — Scores

Humanity’s Last Exam — Academic reasoning, no tools
	•	Gemini 3 Pro 37.5%
	•	Gemini 2.5 Pro 21.6%
	•	Claude Sonnet 4.5 13.7%
	•	GPT-5.1 26.5%

ARC-AGI-2 — Visual reasoning puzzles (ARC Prize Verified)
	•	31.1% — 4.9% — 13.6% — 17.6%

GPOA Diamond — Scientific knowledge, no tools
	•	91.9% — 86.4% — 83.4% — 88.1%

AIME 2025 — Mathematics, no tools
	•	95.0% — 88.0% — 87.0% — 94.0%
	•	A second line shows: 100% — — 100% — —

MathArena Apex — Challenging Math Contest problems
	•	23.4% — 0.5% — 1.6% — 1.0%

MMMU-Pro — Multimodal understanding and reasoning
	•	81.0% — 68.0% — 68.0% — 80.8%

ScreenSpot-Pro — Screen understanding
	•	72.7% — 11.4% — 36.2% — 3.5%

CharXiv Reasoning — Information synthesis from complex charts
	•	81.4% — 69.6% — 68.5% — 69.5%

OmniDocBench 1.5 — OCR (lower is better: Overall Edit Distance)
	•	0.115 — 0.147 — 0.147 — 0.147

Video-MMMU — Knowledge acquisition from videos
	•	87.6% — 83.6% — 77.8% — 80.4%

LiveCodeBench Pro — Competitive coding (Elo rating, higher is better)
	•	2,439 — 1,775 — 1,418 — 2,243

Terminal-Bench 2.0 — Agentic coding (Terminus-2 agent)
	•	54.2% — 32.6% — 42.8% — 47.6%

SWE-Bench Verified — Agentic coding (single attempt)
	•	76.2% — 59.6% — 77.2% — 76.3%

t2-bench — Agentic tool use
	•	85.4% — 54.9% — 84.7% — 80.2%

Vending-Bench 2 — Long-horizon agentic tasks (Net worth, higher is better)
	•	$5,478.16 — $573.64 — $3,838.74 — $1,473.43

FACTS Benchmark Suite — Internal grounding, parametric knowledge, search retrieval
	•	70.5% — 63.4% — 50.4% — 50.8%

SimpleQA Verified — Parametric knowledge
	•	72.1% — 54.5% — 29.3% — 34.9%

MMLU — Multilingual Q&A
	•	91.8% — 89.5% — 89.1% — 91.0%

Global PIQA — Commonsense reasoning across 100+ languages
	•	93.4% — 91.5% — 90.1% — 90.9%

MRCR v2 (8-needle) — Long-context performance
	•	77.0% — 58.0% — 47.1% — 61.6%
	•	Second line: 26.3% — 16.4% — not supported — not supported</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>65</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>9</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">my usual disclaimers:<br><br>- benchmarks are bullshit<br>- look at who they compare against (and who they don’t)<br>- look at what benchmarks they choose (and what they leave out)</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>13</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">Who's not there: GPT-5-Pro, Grok4-4.1, Chinese models<br><br>What's not there: eh, not much. They're pretty confident in this one</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>13</span>
</div>
</div>
<div class="thread-post">
<div class="time-elapsed">1 hour later</div>
<div class="post-text">oh! found an archive link: <a href="https://web.archive.org/web/20251118111103/https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" target="_blank" rel="noopener">web.archive.org/web/20251118...</a></div>
<a href="https://web.archive.org/web/20251118111103/https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">web.archive.org</div>
<div class="embed-title"></div>
<div class="embed-description"></div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>4</span>
</div>
</div>
<div class="thread-post">
<div class="time-elapsed">1 hour later</div>
<div class="post-text">not sure where these numbers came from, but probably accurate</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreif6e664isafopxfzencqsvx4rpkkzvlgp5djcpzqbh4f3c2yghs54@jpeg" alt="Lisan al Gaib @scaling01
X.com
Gemini 3 Pro Preview now on aistudio
Pricing:
<=200K tokens • Input: $2.00 / Output: $12.00
> 200K tokens • Input: $4.00 / Output: $18.00" class="post-image" loading="lazy">
<div class="image-alt">Lisan al Gaib @scaling01
X.com
Gemini 3 Pro Preview now on aistudio
Pricing:
<=200K tokens • Input: $2.00 / Output: $12.00
> 200K tokens • Input: $4.00 / Output: $18.00</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>4</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">Official Gemini 3 Pro launch<br><br><a href="https://blog.google/products/gemini/gemini-3/" target="_blank" rel="noopener">blog.google/products/gem...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreiegqnajhr4l5em6d3ydi7g6jwxxfy6o2gqyfuclrtojbdfapys45i@jpeg" alt="A three-panel bar-chart graphic comparing model performance across three benchmarks: Humanity’s Last Exam, GPQA Diamond, and ARC-AGI-2. All bars are shown in cool shades of blue and gray, with the Gemini 3 Deep Think and Gemini 3 Pro bars in darker blue. Each benchmark section has its title and a subtitle; the first two are labeled “Tools off”, and the third shows a legend for Tools off vs. Tools on, though all displayed bars are tools-off.

⸻

Left panel — Humanity’s Last Exam (Reasoning & knowledge)

Bars from left to right:
	•	Gemini 3 Deep Think: 41% (dark blue)
	•	Gemini 3 Pro: 37.5% (bright blue)
	•	Gemini 2.5 Pro: 21.6% (light blue)
	•	Claude Sonnet 4.5: 13.7% (pale gray)
	•	GPT-5 Pro: 30.7% (medium gray)
	•	GPT-5.1: 26.5% (light gray)

⸻

Middle panel — GPQA Diamond (Scientific knowledge)

Bars from left to right:
	•	Gemini 3 Deep Think: 93.8% (dark blue)
	•	Gemini 3 Pro: 91.9% (bright blue)
	•	Gemini 2.5 Pro: 86.4% (light blue)
	•	Claude Sonnet 4.5: 83.4% (pale gray)
	•	GPT-5 Pro: 88.4% (medium gray)
	•	GPT-5.1: 88.1% (light gray)

⸻

Right panel — ARC-AGI-2 (Visual reasoning puzzles)

Bars from left to right:
	•	Gemini 3 Deep Think: 45.1% (dark blue with a diagonal stripe pattern)
	•	Gemini 3 Pro: 31.1% (bright blue)
	•	Gemini 2.5 Pro: 4.9% (light blue)
	•	Claude Sonnet 4.5: 13.6% (pale gray)
	•	GPT-5 Pro: 15.8% (medium gray)
	•	GPT-5.1: 17.6% (light gray)

⸻

The entire graphic shows Gemini 3 Deep Think scoring highest in all three benchmarks, with Gemini 3 Pro consistently second." class="post-image" loading="lazy">
<div class="image-alt">A three-panel bar-chart graphic comparing model performance across three benchmarks: Humanity’s Last Exam, GPQA Diamond, and ARC-AGI-2. All bars are shown in cool shades of blue and gray, with the Gemini 3 Deep Think and Gemini 3 Pro bars in darker blue. Each benchmark section has its title and a subtitle; the first two are labeled “Tools off”, and the third shows a legend for Tools off vs. Tools on, though all displayed bars are tools-off.

⸻

Left panel — Humanity’s Last Exam (Reasoning & knowledge)

Bars from left to right:
	•	Gemini 3 Deep Think: 41% (dark blue)
	•	Gemini 3 Pro: 37.5% (bright blue)
	•	Gemini 2.5 Pro: 21.6% (light blue)
	•	Claude Sonnet 4.5: 13.7% (pale gray)
	•	GPT-5 Pro: 30.7% (medium gray)
	•	GPT-5.1: 26.5% (light gray)

⸻

Middle panel — GPQA Diamond (Scientific knowledge)

Bars from left to right:
	•	Gemini 3 Deep Think: 93.8% (dark blue)
	•	Gemini 3 Pro: 91.9% (bright blue)
	•	Gemini 2.5 Pro: 86.4% (light blue)
	•	Claude Sonnet 4.5: 83.4% (pale gray)
	•	GPT-5 Pro: 88.4% (medium gray)
	•	GPT-5.1: 88.1% (light gray)

⸻

Right panel — ARC-AGI-2 (Visual reasoning puzzles)

Bars from left to right:
	•	Gemini 3 Deep Think: 45.1% (dark blue with a diagonal stripe pattern)
	•	Gemini 3 Pro: 31.1% (bright blue)
	•	Gemini 2.5 Pro: 4.9% (light blue)
	•	Claude Sonnet 4.5: 13.6% (pale gray)
	•	GPT-5 Pro: 15.8% (medium gray)
	•	GPT-5.1: 17.6% (light gray)

⸻

The entire graphic shows Gemini 3 Deep Think scoring highest in all three benchmarks, with Gemini 3 Pro consistently second.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>7</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>2</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">this feels like the headline</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreiab5udpfp6yumhn4457jz7drejt64z6qdk4hp3r6u4muik5svaflq@jpeg" alt="It's state-of-the-art in reasoning, built to grasp depth and nuance — whether it's perceiving the subtle clues in a creative idea, or peeling apart the overlapping layers of a difficult problem. Gemini 3 is also much better at figuring out the context and intent behind your request, so you get what you need with less prompting. It's amazing to think that in just two years, Al has evolved from simply reading text and images to reading the room." class="post-image" loading="lazy">
<div class="image-alt">It's state-of-the-art in reasoning, built to grasp depth and nuance — whether it's perceiving the subtle clues in a creative idea, or peeling apart the overlapping layers of a difficult problem. Gemini 3 is also much better at figuring out the context and intent behind your request, so you get what you need with less prompting. It's amazing to think that in just two years, Al has evolved from simply reading text and images to reading the room.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>13</span>
</div>
</div>