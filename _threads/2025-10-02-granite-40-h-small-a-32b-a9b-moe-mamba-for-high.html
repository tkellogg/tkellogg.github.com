---
layout: thread
title: "Granite-4.0-H-Small: a 32B-A9B MoE Mamba for high efficency"
date: 2025-10-02 15:21:53 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3m27u7sfxzk2q
likes: 31
reposts: 2
post_count: 3
summary: "Granite-4.0-H-Small: a 32B-A9B MoE Mamba for high efficency  Damn! IBM is on the map. The American Qwen? I barely even knew IBM made LLMs, this is sol..."
similar:
  - url: "/threads/2025-10-28-ibm-is-cooking-apparently-granite-4-nano-is-a-1/"
    title: "IBM is cooking, apparently "
  - url: "/threads/2025-12-04-its-the-year-of-our-lord-2025-and-auth-is-still-n/"
    title: "it’s the year of our lord 2025 and auth is still near impossible"
  - url: "/threads/2025-08-05-gpt-oss-openais-open-weights-model-120b-20b-v/"
    title: "gpt-oss, OpenAI's open weights model"
---
<div class="thread-post">
<div class="post-text">Granite-4.0-H-Small: a 32B-A9B MoE Mamba for high efficency<br><br>Damn! IBM is on the map. The American Qwen? I barely even knew IBM made LLMs, this is solid<br><br><a href="https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models" target="_blank" rel="noopener">www.ibm.com/new/announce...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreifobyytbu2fidshetpkpivj34rhefgok33ryfhafgii22rlb46tmq@jpeg" alt="The bar chart is titled **“Retrieval Augmented Generation (RAG)”** and shows **MTRAG mean accuracy** on the y-axis (0–80 scale).

### Results by model:

* **Granite-4.0-H-Small**: **73** (blue bar, highest)
* **Granite-4.0-Micro**: **72** (blue bar, nearly tied with H-Small)
* **GPT-OSS-20B**: **68** (green bar)
* **Mistral-Small-3.2-Instruct**: **48** (green bar, lowest score)
* **Llama-3.2-Instruct**: **53** (green bar)
* **Llama-3.3-70B-Instruct**: **61** (green bar)
* **Qwen3-8B**: **55** (green bar)

### Key takeaway:

The **Granite-4.0 models (H-Small and Micro)** outperform all others, achieving ~73 accuracy, with GPT-OSS-20B in third at 68. The weakest performance is from **Mistral-Small-3.2-Instruct (48)**.
" class="post-image" loading="lazy">
<div class="image-alt">The bar chart is titled **“Retrieval Augmented Generation (RAG)”** and shows **MTRAG mean accuracy** on the y-axis (0–80 scale).

### Results by model:

* **Granite-4.0-H-Small**: **73** (blue bar, highest)
* **Granite-4.0-Micro**: **72** (blue bar, nearly tied with H-Small)
* **GPT-OSS-20B**: **68** (green bar)
* **Mistral-Small-3.2-Instruct**: **48** (green bar, lowest score)
* **Llama-3.2-Instruct**: **53** (green bar)
* **Llama-3.3-70B-Instruct**: **61** (green bar)
* **Qwen3-8B**: **55** (green bar)

### Key takeaway:

The **Granite-4.0 models (H-Small and Micro)** outperform all others, achieving ~73 accuracy, with GPT-OSS-20B in third at 68. The weakest performance is from **Mistral-Small-3.2-Instruct (48)**.
</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>31</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>2</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">initial take: okay, it can work as a model. it doesn't have a fun personality, but that's kinda what you get with a 32b-a9b. fwiw it doesn't hallucinate too badly afaict<br><br>could be worth looking into if you're thinking about RL'ing LoRAs, it's probably malleable</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>2</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">also: Mambas aren’t transformers, so this isn’t technically an LLM, whatever..</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>1</span>
</div>
</div>