---
layout: thread
title: "ğŸš¨New DeepSeek Model IncomingğŸš¨"
date: 2025-04-04 10:45:21 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3llyaenztg225
likes: 30
reposts: 6
post_count: 2
summary: "ğŸš¨New DeepSeek Model IncomingğŸš¨  but first they release the paper describing generative reward modeling (GRM) via Self-Principled Critique Tuning (SPCT)..."
similar:
  - url: "/threads/2025-12-01-deepseek-32-2-new-models-32-a-open-weight/"
    title: "DeepSeek 3.2 "
  - url: "/threads/2025-09-16-qwen-tongyi-deep-research-a-32b-model-that-beats/"
    title: "Qwen Tongyi Deep Research"
  - url: "/threads/2025-10-27-minimax-open-sources-m2-this-model-has-been-shaki/"
    title: "MiniMax open sources M2"
---
<div class="thread-post">
<div class="post-text">ğŸš¨New DeepSeek Model IncomingğŸš¨<br><br>but first they release the paper describing generative reward modeling (GRM) via Self-Principled Critique Tuning (SPCT)<br><br>looking forward to DeepSeek-GRM!<br><br><a href="https://arxiv.org/abs/2504.02495" target="_blank" rel="noopener">arxiv.org/abs/2504.02495</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreida7l2d7a5cst7qijk7q55tudqgtgdzd5kvn64u3pg3hpozj6ybpa@jpeg" alt="A line chart titled â€œFigure 1: Inference-time scaling performance with different RMs on all tested RM benchmarksâ€ shows performance on the y-axis (ranging from 66.5 to 72.5) and k: #sampled rewards (logscale) on the x-axis, with values from 1 to 32.

Key observations:
	â€¢	DeepSeek-GRM-27B (MetaRM@k) (Ours) is the top performer, shown with a red line and star markers, rising steeply and leveling near 72.5.
	â€¢	DeepSeek-GRM-27B (Voting@k) (Ours) follows, in blue with stars, peaking slightly above 70.5.
	â€¢	GPT-4o (Greedy) is shown as a gray dashed line, sitting just under 71.
	â€¢	Other models, shown in orange, green, brown, and gray lines (scalar or voting methods), plateau between ~66.5 and ~68.5.
	â€¢	LLM-as-a-Judge w/ TokenProb, Skywork-Reward-Gemma-2-27B, and DeepSeek-BTRM-27B are among these lower-performing models.

Caption summary: The plot shows how performance scales with the number of reward samples at inference time. Results are up to 8 samples, with some (DeepSeek models) extrapolated to 32. Models in non-italic font use Gemma-2-27B as their base." class="post-image" loading="lazy">
<div class="image-alt">A line chart titled â€œFigure 1: Inference-time scaling performance with different RMs on all tested RM benchmarksâ€ shows performance on the y-axis (ranging from 66.5 to 72.5) and k: #sampled rewards (logscale) on the x-axis, with values from 1 to 32.

Key observations:
	â€¢	DeepSeek-GRM-27B (MetaRM@k) (Ours) is the top performer, shown with a red line and star markers, rising steeply and leveling near 72.5.
	â€¢	DeepSeek-GRM-27B (Voting@k) (Ours) follows, in blue with stars, peaking slightly above 70.5.
	â€¢	GPT-4o (Greedy) is shown as a gray dashed line, sitting just under 71.
	â€¢	Other models, shown in orange, green, brown, and gray lines (scalar or voting methods), plateau between ~66.5 and ~68.5.
	â€¢	LLM-as-a-Judge w/ TokenProb, Skywork-Reward-Gemma-2-27B, and DeepSeek-BTRM-27B are among these lower-performing models.

Caption summary: The plot shows how performance scales with the number of reward samples at inference time. Results are up to 8 samples, with some (DeepSeek models) extrapolated to 32. Models in non-italic font use Gemma-2-27B as their base.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>30</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>6</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">one trick they used was to replace scalar grades with critiquing the source<br><br>which, yeah, that does seem like it would help</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreiakhgtzmvw5pckfwsbduys7bvvmtyva2auzptua2hj2lbszmzlq5a@jpeg" alt="A complex diagram titled â€œFigure 3: Illustration of SPCTâ€ (likely referring to a Structured Principles-Centric Training method) explains a multi-stage process involving RFT (Rejective Fine-Tuning), RL (Reinforcement Learning), and Inference, using GRMs (General Reward Models) and principle-guided scoring.

Top section (RFT):
	â€¢	GRM samples responses using Q&R (question & response) and evaluates using predefined principles like â€œInstruction Adherence,â€ â€œSafety,â€ â€œClarity,â€ and â€œRelevance.â€
	â€¢	Scored responses are extracted and categorized (e.g. 2/10, 4/10; 6/10, 1/10) for training rule and reward modules.
	â€¢	Some responses are flagged as â€œToo Easy/Incorrect.â€

Middle section (RL):
	â€¢	GRM rolls out responses with principles like â€œLogic Chain Correctnessâ€ and â€œCompleteness.â€
	â€¢	Responses are compared and scored for rule/reward extraction, leading to online or offline updates.

Bottom section (Inference):
	â€¢	Uses parallel sampling of responses via GRM.
	â€¢	A wide variety of principles guide evaluation: â€œTechnical Accuracy,â€ â€œPractical Implementation,â€ â€œLanguage Proficiency,â€ â€œEngagement,â€ etc., each with specific weights.
	â€¢	Critiques are shown for each comparison, and final scores are computed for the responses.
	â€¢	Scores from multiple sets are collected and passed to a Voting module or a Meta RM which combines multiple critiques to produce final Rewards.

Right side:
	â€¢	Color-coded scores show how different responses (1 to 4) fare across principles.
	â€¢	Voting simply aggregates outcomes (e.g., 17/40), while Meta RM uses weighted reasoning from critiques across principles (e.g., 5/20, 13/20).

Caption summary:
SPCT uses a principle-based framework for fine-tuning and reinforcement learning, guiding inference-time behavior. Naive voting and Meta RM approaches scale critique-guided scoring, providing nuanced reward signals across a broad value space. The formal equation below represents the principle and critique generation process." class="post-image" loading="lazy">
<div class="image-alt">A complex diagram titled â€œFigure 3: Illustration of SPCTâ€ (likely referring to a Structured Principles-Centric Training method) explains a multi-stage process involving RFT (Rejective Fine-Tuning), RL (Reinforcement Learning), and Inference, using GRMs (General Reward Models) and principle-guided scoring.

Top section (RFT):
	â€¢	GRM samples responses using Q&R (question & response) and evaluates using predefined principles like â€œInstruction Adherence,â€ â€œSafety,â€ â€œClarity,â€ and â€œRelevance.â€
	â€¢	Scored responses are extracted and categorized (e.g. 2/10, 4/10; 6/10, 1/10) for training rule and reward modules.
	â€¢	Some responses are flagged as â€œToo Easy/Incorrect.â€

Middle section (RL):
	â€¢	GRM rolls out responses with principles like â€œLogic Chain Correctnessâ€ and â€œCompleteness.â€
	â€¢	Responses are compared and scored for rule/reward extraction, leading to online or offline updates.

Bottom section (Inference):
	â€¢	Uses parallel sampling of responses via GRM.
	â€¢	A wide variety of principles guide evaluation: â€œTechnical Accuracy,â€ â€œPractical Implementation,â€ â€œLanguage Proficiency,â€ â€œEngagement,â€ etc., each with specific weights.
	â€¢	Critiques are shown for each comparison, and final scores are computed for the responses.
	â€¢	Scores from multiple sets are collected and passed to a Voting module or a Meta RM which combines multiple critiques to produce final Rewards.

Right side:
	â€¢	Color-coded scores show how different responses (1 to 4) fare across principles.
	â€¢	Voting simply aggregates outcomes (e.g., 17/40), while Meta RM uses weighted reasoning from critiques across principles (e.g., 5/20, 13/20).

Caption summary:
SPCT uses a principle-based framework for fine-tuning and reinforcement learning, guiding inference-time behavior. Naive voting and Meta RM approaches scale critique-guided scoring, providing nuanced reward signals across a broad value space. The formal equation below represents the principle and critique generation process.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>4</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>