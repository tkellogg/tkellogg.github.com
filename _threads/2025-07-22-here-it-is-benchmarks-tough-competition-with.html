---
layout: thread
title: "here it is:"
date: 2025-07-22 21:44:37 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lulhzupjbc2a
likes: 34
reposts: 1
post_count: 2
summary: "here it is:  * benchmarks: tough competition with Sonnet-4 * 256K context, expandable to 1M with YaRN  there’s also a CLI forked from gemini-cli  qwen..."
similar:
  - url: "/threads/2025-09-11-qwen3-next-80b-a3b-base-instruct-thinking-pe/"
    title: "Qwen3-Next-80B-A3B Base, Instruct & Thinking"
  - url: "/threads/2025-08-06-qwen3-4b-instruct-thinking-uuuh-guys-this-isn/"
    title: "Qwen3-4B Instruct & Thinking"
  - url: "/threads/2025-09-16-qwen-tongyi-deep-research-a-32b-model-that-beats/"
    title: "Qwen Tongyi Deep Research"
---
<div class="thread-post">
<div class="post-text">here it is:<br><br>* benchmarks: tough competition with Sonnet-4<br>* 256K context, expandable to 1M with YaRN<br><br>there’s also a CLI forked from gemini-cli<br><br><a href="https://qwenlm.github.io/blog/qwen3-coder/" target="_blank" rel="noopener">qwenlm.github.io/blog/qwen3-c...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidjzjj34hp2u2kxtdg5n3nu766ndvqymxj6wh3gup2opobj2fxmvu@jpeg" alt="A terminal-style performance comparison table highlights the benchmark results for multiple models across three categories: Agentic Coding, Agentic Browser Use, and Agentic Tool Use. The title “QWEN3-CODER” is shown in large pixel-style font. The focal model, Qwen3-Coder 480B-A35B-Instruct, is shaded in orange for emphasis.

Agentic Coding Benchmarks (Top Section)
	•	Qwen3-Coder scores highest or close to highest across nearly all agentic coding tasks:
	•	Terminal-Bench: 37.5
	•	SWE-bench Verified: 69.6
	•	w/ OpenHands, 500 turns: 67.0
	•	w/ Private Scaffolding: 61.8
	•	Spider2: 31.1
	•	Other models like Claude Sonnet-4 (70.4 Terminal-Bench, 68.0 SWE-bench Verified) and GPT-4.1 (63.8 SWE-bench Live) show strong performance on individual tasks.

Agentic Browser Use Benchmarks (Middle Section)
	•	Qwen3-Coder leads with:
	•	WebArena: 49.9
	•	Mind2Web: 55.8
	•	Again outperforming other models including GPT-4.1 and Claude Sonnet-4.

Agentic Tool Use Benchmarks (Bottom Section)
	•	Qwen3-Coder tops in:
	•	BFCL-v3: 68.7
	•	TAU-Bench Retail: 77.5
	•	TAU-Bench Airline: 60.0
	•	Competing strongly with Claude Sonnet-4 (BFCL-v3: 73.3, Retail: 80.5).

All scores are numerical performance metrics (exact units not specified), and the model list includes both open and proprietary models. Graphical elements like a gradient top border and dark background give a retro terminal/arcade vibe. The terminal bar shows battery and network activity." class="post-image" loading="lazy">
</div>
</div>
<a href="https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lulgd64xjc2a" class="post-quote" target="_blank" rel="noopener">
<div class="quote-header">
<img src="https://cdn.bsky.app/img/avatar/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidvgcq72e5erl4stnap6wjzas6a2wburoa7yzctwuy4vgx4vb5fsi@jpeg" alt="" class="quote-avatar">
<span class="quote-author">Tim Kellogg</span>
<span class="quote-handle">@timkellogg.me</span>
</div>
<div class="quote-text">Alibaba is likely to drop “Qwen3-Coder-480B-A35B-Instruct” in a few hours<br><br>in the i18n files for their mobile app they describe it as:<br><br>"a powerful coding-specialized language model excelling in code generation, tool use, and agentic tasks”<br><br>should come “tonight” (relative to China)</div>

</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>34</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">on the inside:<br><br>* shallower than Qwen3 (62 vs 94 layers)<br>* more experts (160 vs 128, in direction of K2)<br>* more attention heads (96 vs 64, opposite of K2)<br><br>curious what the thought is..</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>5</span>
</div>
</div>