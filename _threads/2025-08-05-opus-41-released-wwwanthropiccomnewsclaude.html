---
layout: thread
title: "Opus 4.1 Released"
date: 2025-08-05 16:52:33 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lvo6ajgb2c2m
likes: 22
reposts: 2
post_count: 2
summary: "Opus 4.1 Released  www.anthropic.com/news/claude-..."
similar:
  - url: "/threads/2025-11-24-opus-45-now-13rd-the-cost-and-sota-in-programm/"
    title: "Opus 4.5"
  - url: "/threads/2025-12-02-claude-opus-soul-document-opus-45-was-indeed/"
    title: "Claude Opus “soul document”"
  - url: "/threads/2025-11-24-anthropic-has-no-competitors-because-nobody-else/"
    title: "Anthropic has no competitors, because nobody else sells Claude"
---
<div class="thread-post">
<div class="post-text">Opus 4.1 Released<br><br><a href="https://www.anthropic.com/news/claude-opus-4-1" target="_blank" rel="noopener">www.anthropic.com/news/claude-...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreid7uhceyuzkb42cooabou2c4qxzinytv4p2nwu22lvahyhgjhxl2m@jpeg" alt="Bar chart comparing the accuracy of two AI models on the SWE-bench Verified benchmark, labeled “Software engineering.”
	•	The y-axis is labeled “ACCURACY” and ranges from 50 to 80.
	•	Two vertical bars are shown:
	•	Left bar (light beige) represents Opus 4 (May 2025) with an accuracy of 72.5%
	•	Right bar (darker orange) represents Opus 4.1 (Aug 2025) with an accuracy of 74.5%
	•	The Opus 4.1 bar is slightly taller, indicating an improvement of 2 percentage points over Opus 4." class="post-image" loading="lazy">
<div class="image-alt">Bar chart comparing the accuracy of two AI models on the SWE-bench Verified benchmark, labeled “Software engineering.”
	•	The y-axis is labeled “ACCURACY” and ranges from 50 to 80.
	•	Two vertical bars are shown:
	•	Left bar (light beige) represents Opus 4 (May 2025) with an accuracy of 72.5%
	•	Right bar (darker orange) represents Opus 4.1 (Aug 2025) with an accuracy of 74.5%
	•	The Opus 4.1 bar is slightly taller, indicating an improvement of 2 percentage points over Opus 4.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>22</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>2</span>
</div>
</div>
<div class="thread-post">
<div class="time-elapsed">4 hours later</div>
<div class="post-text">Opus 4.1 seems like a incremental improvement, mostly across the board on benchmarks <br><br>i’ve heard chatter that it’s a bit of a regression. i suppose that’s inevitable given only marginal improvements <br><br>still, i’m excited to push its agency a bit more</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreiblq6ud6kp4q3pwkhangv4xvi37kbhxeffuc46vx7cl3ctlwaysyy@jpeg" alt="Performance comparison table of AI models across seven benchmarks. The columns represent different models: Claude Opus 4.1, Claude Opus 4, Claude Sonnet 4, OpenAI o3, and Gemini 2.5 Pro. Claude Opus 4.1 is highlighted with a red border.

Task	Claude Opus 4.1	Claude Opus 4	Claude Sonnet 4	OpenAI o3	Gemini 2.5 Pro
Agentic coding (SWE-bench Verified)	74.5%	72.5%	72.7%	69.1%	67.2%
Agentic terminal coding (Terminal-Bench)	43.3%	39.2%	35.5%	30.2%	25.3%
Graduate-level reasoning (GPQA Diamond)	80.9%	79.6%	75.4%	83.3%	86.4%
Agentic tool use (TAU-bench, Retail / Airline)	82.4% / 56.0%	81.4% / 59.6%	80.5% / 60.0%	70.4% / 52.0%	—
Multilingual Q&A (MMLU, 5x non-English avg)	89.5%	88.8%	86.5%	88.8%	—
Visual reasoning (MMMU, validation)	77.1%	76.5%	74.4%	82.9%	82.0%
High school math competition (AIME 2025)	78.0%	75.5%	70.5%	88.9%	88.0%

Notes:
	•	Claude 4.1 leads in agentic coding and multilingual Q&A.
	•	Gemini 2.5 Pro leads in graduate-level reasoning and high school math.
	•	OpenAI o3 has top scores in visual reasoning and math.
	•	Claude models dominate in agentic tool use." class="post-image" loading="lazy">
<div class="image-alt">Performance comparison table of AI models across seven benchmarks. The columns represent different models: Claude Opus 4.1, Claude Opus 4, Claude Sonnet 4, OpenAI o3, and Gemini 2.5 Pro. Claude Opus 4.1 is highlighted with a red border.

Task	Claude Opus 4.1	Claude Opus 4	Claude Sonnet 4	OpenAI o3	Gemini 2.5 Pro
Agentic coding (SWE-bench Verified)	74.5%	72.5%	72.7%	69.1%	67.2%
Agentic terminal coding (Terminal-Bench)	43.3%	39.2%	35.5%	30.2%	25.3%
Graduate-level reasoning (GPQA Diamond)	80.9%	79.6%	75.4%	83.3%	86.4%
Agentic tool use (TAU-bench, Retail / Airline)	82.4% / 56.0%	81.4% / 59.6%	80.5% / 60.0%	70.4% / 52.0%	—
Multilingual Q&A (MMLU, 5x non-English avg)	89.5%	88.8%	86.5%	88.8%	—
Visual reasoning (MMMU, validation)	77.1%	76.5%	74.4%	82.9%	82.0%
High school math competition (AIME 2025)	78.0%	75.5%	70.5%	88.9%	88.0%

Notes:
	•	Claude 4.1 leads in agentic coding and multilingual Q&A.
	•	Gemini 2.5 Pro leads in graduate-level reasoning and high school math.
	•	OpenAI o3 has top scores in visual reasoning and math.
	•	Claude models dominate in agentic tool use.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>7</span>
</div>
</div>