---
layout: thread
title: "DeepSeek-OCR"
date: 2025-10-20 11:12:29 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3m3moofx76s2q
likes: 52
reposts: 2
post_count: 4
summary: "DeepSeek-OCR  a tiny 3B-A0.5B MoE OCR model that runs fast on a single A100 40GB with very high precision and excellent compression  why it‚Äôs cool ‚Äî t..."
similar:
  - url: "/threads/2025-01-27-alert-deepseek-janus-pro-7b-its-multimodal-an/"
    title: "üêã Alert! DeepSeek Janus-Pro-7B"
  - url: "/threads/2025-10-21-recently-we-got-1-deepseek-sparse-attention-ds/"
    title: "recently we got "
  - url: "/threads/2025-05-29-holy-cow-an-8b-comparing-to-o3-mini/"
    title: "holy cow, an 8b comparing to o3-mini"
---
<div class="thread-post">
<div class="post-text">DeepSeek-OCR<br><br>a tiny 3B-A0.5B MoE OCR model that runs fast on a single A100 40GB with very high precision and excellent compression<br><br>why it‚Äôs cool ‚Äî they use images as a way to compress text and get around the O(n^2)<br><br><a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR" target="_blank" rel="noopener">huggingface.co/deepseek-ai/...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreifnhyjbf2oa43evggfjums7h2i43vqswdvdjeuslran5kcxk3cfrq@jpeg" alt="A scatter plot titled ‚ÄúOverall Performance (Edit Distance) vs Average Vision Tokens per Image‚Äù compares OCR and vision-language models by token efficiency and accuracy.

Axes:
	‚Ä¢	X-axis: Average Vision Tokens per Image (log scale, decreases left to right).
	‚Ä¢	Y-axis: Overall Performance (Edit Distance) ‚Äî lower values indicate better accuracy.

‚∏ª

Color legend (bottom-left):
	‚Ä¢	üî¥ DeepEncoder Series
	‚Ä¢	üü© QwenEncoder Series
	‚Ä¢	üîµ InternVLEncoder Series
	‚Ä¢	üüß Other Encoders

‚∏ª

Highlighted regions:
	‚Ä¢	Left (purple box): ‚ÄúVision Tokens > 1500, Average per image (‚Üê More)‚Äù
	‚Ä¢	Right (blue box): ‚ÄúVision Tokens < 1000, Average per image (‚Üí Fewer)‚Äù
	‚Ä¢	Green box: ‚ÄúHigh Accuracy ED < 0.25 (‚Üë better)‚Äù

‚∏ª

Key models:

DeepEncoder Series (red circles):
	‚Ä¢	DeepSeek-OCR (Large, Base, Small, Tiny, Gundam, Gundam-M 200dpi) ‚Äî clustered near the top-right with high accuracy (‚âà0.1‚Äì0.25 ED).
	‚Ä¢	DeepSeek-OCR (Gundam-M 200dpi) achieves the best performance.

QwenEncoder Series (green squares):
	‚Ä¢	dots.ocr, Qwen2.5-VL-72B, OCRFlux-3B, Qwen2.5-VL-7B, OLMOCR ‚Äî around mid-range (0.25‚Äì0.4 ED) with 1000‚Äì5000 tokens per image.
	‚Ä¢	dots.ocr (200dpi) is among the top in this group.

InternVLEncoder Series (blue triangles):
	‚Ä¢	InternVL2-76B, InternVL3-78B, MinerU2.0 ‚Äî higher token usage (4000‚Äì7000) with moderate accuracy (0.2‚Äì0.45 ED).

Other Encoders (orange diamonds):
	‚Ä¢	GOT-OCR2.0 (mid performance)
	‚Ä¢	SmolDocling (bottom-right, 400 tokens/image, lowest accuracy ‚âà0.5 ED).

‚∏ª

Summary:

Models using fewer vision tokens (right side) generally have worse accuracy, while those with more tokens per image (left side) perform better.
DeepSeek-OCR (Gundam-M 200dpi) leads overall in accuracy, while SmolDocling is the smallest and least accurate." class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>52</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>2</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">instead of focusing only on accuracy, they also focus on visual compression <br><br>‚Äúfor a document containing 1000 words, how many vision tokens are at least needed for decoding? This question holds significant importance for research in the principle that ‚Äòa picture is worth a thousand words.‚Äô"</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreicyds7ddugh4mfoqfae6yjsxgpwi75l4f7ndwlnab5w4vrcwjdkim@jpeg" alt="Figure 3 | The architecture of DeepSeek-OCR.

The diagram illustrates the processing pipeline of DeepSeek-OCR, which consists of a DeepEncoder and a DeepSeek-3B-MoE decoder.

‚∏ª

Flow:
	1.	Input: A document image (left) is divided into n√ó16√ó16 patches.
	2.	Tokenizer (DeepEncoder):
	‚Ä¢	SAM (ViTDet 80M) ‚Äî applies local attention for perception and segmentation of visual structure.
	‚Ä¢	Conv (16√ó) ‚Äî downsamples patches into vision tokens (n/16).
	‚Ä¢	CLIP (ViT 300M) ‚Äî applies global attention to derive dense semantic embeddings.
Together, these form the DeepEncoder, with an embedding layer bridging local and global attention.
	3.	Decoder:
The encoded vision tokens and prompt are passed to the DeepSeek-3B (MOE-A570M) decoder, which generates the output sequence (text or OCR tokens).

‚∏ª

Caption text (verbatim):

Figure 3 | The architecture of DeepSeek-OCR. DeepSeek-OCR consists of a DeepEncoder and a DeepSeek-3B-MoE decoder. DeepEncoder is the core of DeepSeek-OCR, comprising three components: a SAM [17] for perception dominated by window attention, a CLIP [29] for knowledge with dense global attention, and a 16√ó token compressor that bridges between them." class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>12</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">Textual Forgetting<br><br>An advantage of using images to represent text is you can reduce the resolution, ‚Äúforgetting‚Äù things that happened further in the past<br><br>The implication is that you could scale this up to very long (text) contexts</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreicq3o7b4vuaaa5elg74fqb2eepvbklx5mfa57hzxz22nsthp26c3u@jpeg" alt="Figure 13 | The diagram illustrates parallels between memory decay, visual distance, and text compression as processes of progressive information loss.

Three horizontal scales are shown, each moving from ‚ÄúCrystal Clear‚Äù on the left to ‚ÄúAlmost Gone‚Äù on the right, with corresponding examples:

‚∏ª

Memory
	‚Ä¢	Icons: A brain inside a lightbulb.
	‚Ä¢	Labeled points:
	‚Ä¢	Just happened ‚Üí 1 hour ‚Üí 1 day ‚Üí 1 week ‚Üí 1 month ‚Üí 1 year
	‚Ä¢	Clarity decreases over time (Crystal Clear ‚Üí Very Blurry ‚Üí Almost Gone).
	‚Ä¢	Axis: Time ‚Üí

‚∏ª

Vision
	‚Ä¢	Icons: An eye.
	‚Ä¢	Labeled points:
	‚Ä¢	10 cm ‚Üí 50 cm ‚Üí 1 m ‚Üí 3 m ‚Üí 10 m ‚Üí 20 m
	‚Ä¢	As distance increases, objects become blurrier.
	‚Ä¢	Axis: Distance ‚Üë

‚∏ª

Text
	‚Ä¢	Icons: A document.
	‚Ä¢	Labeled points:
	‚Ä¢	Text token ‚Üí Gundam ‚Üí Large ‚Üí Base ‚Üí Small ‚Üí Tiny
	‚Ä¢	Text becomes progressively less clear as resolution decreases.
	‚Ä¢	Axis: Resolution ‚Üì

‚∏ª

Caption (verbatim):
Figure 13 | Forgetting mechanisms constitute one of the most fundamental characteristics of human memory. The contexts optical compression approach can simulate this mechanism by rendering previous rounds of historical text onto images for initial compression, then progressively resizing older images to achieve multi-level compression, where token counts gradually decrease and text becomes increasingly blurred, thereby accomplishing textual forgetting." class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>12</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">This paper is disorienting to me. I‚Äôm not sure if it‚Äôs a revolutionary breakthrough or bullshit. I‚Äôm leaning towards the first<br><br>The question seems to be if this can scale up to >1T<br><br>But also, by processing text via images, helpful inline diagrams are actually helpful<br><br>Time will tell</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>15</span>
</div>
</div>