---
layout: thread
title: "gemma3n is now open source, available everywhere — huggingface, transformers,..."
date: 2025-06-26 21:39:59 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lsk3nohxjc2i
likes: 22
reposts: 3
post_count: 2
summary: "gemma3n is now open source, available everywhere — huggingface, transformers, gguf, ollama, mlx, etc.  huggingface.co/blog/gemma3n"
similar:
  - url: "/threads/2025-04-28-its-here-a-real-qwen3-model-huggingfacecoqwe/"
    title: "it’s here! a real Qwen3 model "
  - url: "/threads/2025-05-20-gemma-3n-the-4b-llm-thats-up-with-sonnet-37-in/"
    title: "Gemma 3n: the 4b LLM that’s up with sonnet-3.7 in chatbot arena"
  - url: "/threads/2025-10-16-gemma-27b-variant-discovered-a-new-cancer-pathway/"
    title: "Gemma 27B variant discovered a new cancer pathway treatment that has been val..."
---
<div class="thread-post">
<div class="post-text">gemma3n is now open source, available everywhere — huggingface, transformers, gguf, ollama, mlx, etc.<br><br><a href="https://huggingface.co/blog/gemma3n" target="_blank" rel="noopener">huggingface.co/blog/gemma3n</a></div>
<a href="https://huggingface.co/blog/gemma3n" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">huggingface.co</div>
<div class="embed-title">Gemma 3n fully available in the open-source ecosystem!</div>
<div class="embed-description">We’re on a journey to advance and democratize artificial intelligence through open source and open science.</div>
</div>
</a>
<a href="https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lpmvrw5dj22s" class="post-quote" target="_blank" rel="noopener">
<div class="quote-header">
<img src="https://cdn.bsky.app/img/avatar/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidvgcq72e5erl4stnap6wjzas6a2wburoa7yzctwuy4vgx4vb5fsi@jpeg" alt="" class="quote-avatar">
<span class="quote-author">Tim Kellogg</span>
<span class="quote-handle">@timkellogg.me</span>
</div>
<div class="quote-text">Gemma 3n: the 4b LLM that’s up with sonnet-3.7 in chatbot arena<br><br>the new innovation is Per-Layer Embeddings, which let it consume dramatically less memory<br><br>it was created for phones, and is being rolled out to Android phones soon<br><br>developers.googleblog.com/en/introduci...</div>
<div class="quote-images"><img src="https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreibkyock4gyt6ih4qikgqj6ufrnenugdybtf6mcncuywuktqjt5gem@jpeg" alt="Bar chart titled “Chatbot Arena Elo Score” compares five chatbot models ranked by performance. Each vertical bar represents a model and its Elo score:
	•	Claude 3.7 Sonnet scores highest at 1287 (Proprietary).
	•	Gemma 3n is second with 1283, shown in a glowing blue gradient bar. A note clarifies it is a 4B model, with 1.4B active parameters using PLE caching and a total of 7B parameters.
	•	GPT-4.1-nano-2025-04-14 follows with 1268 (Proprietary).
	•	Llama-4-Maverick-17B-128E-Instruct has a score of 1266, with 17B parameters and a MoE (Mixture-of-Experts) configuration.
	•	Phi 4 ranks last at 1202 and is listed with 14B parameters.

Footnote at bottom notes scores are as of May 19, 2025, with Gemma 3n’s confidence interval listed as ±11." class="quote-image" loading="lazy"></div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>22</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="time-elapsed">17 hours later</div>
<div class="post-text">i had o3 line up the comparison of gemma 3n with other models of varying sizes</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidmwqfr7iv25ktkiqesukev3yvr452v6fhe22kr6ch4ouknpexpnq@jpeg" alt="Alt text:
Grouped bar chart titled “Benchmark Comparison Across Models.” The x-axis lists seven language models (left → right): Gemma 3-n E4B IT, GPT-4o, Claude 3 Opus, Llama 3.3 70B, Mixtral 8×22B, Llama 3 8B, Phi-3 mini 4B.
For each model there are three adjacent vertical bars:

Model	MMLU Accuracy	MBPP pass@1	HumanEval pass@1
Gemma 3-n E4B IT	~65 %	~64 %	~75 %
GPT-4o	~86 %	~71 %	~86 %
Claude 3 Opus	~89 %	~70 %	~86 %
Llama 3.3 70B	~86 %	~88 %	~88 %
Mixtral 8×22B	~78 %	~64 %	~74 %
Llama 3 8B	~73 %	~73 %	~73 %
Phi-3 mini 4B	~69 %	~70 %	~59 %

The y-axis spans 0 % to 100 %. Taller bars indicate stronger performance; Llama 3.3 70B, GPT-4o, and Claude 3 Opus lead across all three benchmarks, while Gemma 3-n E4B IT and Phi-3 mini trail, especially on HumanEval." class="post-image" loading="lazy">
</div>
</div>
</div>