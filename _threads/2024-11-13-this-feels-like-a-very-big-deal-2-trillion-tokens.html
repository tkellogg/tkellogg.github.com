---
layout: thread
title: "this feels like a very big deal"
date: 2024-11-13 19:44:52 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lau4cnxfhc2h
likes: 21
reposts: 5
post_count: 2
summary: "this feels like a very big deal  2 trillion tokens of permissively licensed text & code, so you can train (actually) open LLMs  and data acquisition i..."
similar:
  - url: "/threads/2025-07-06-new-3-token-attention-reduces-pre-training-data-re/"
    title: "new 3-token attention reduces pre-training data requirements"
  - url: "/threads/2024-12-26-not-enough-is-being-said-about-deepseeks-multi-to/"
    title: "not enough is being said about DeepSeek’s multi token prediction (MTP)"
  - url: "/threads/2025-07-08-smollm3-a-highly-detailed-look-into-modern-model/"
    title: "SmolLM3: a highly detailed look into modern model training"
---
<div class="thread-post">
<div class="post-text">this feels like a very big deal<br><br>2 trillion tokens of permissively licensed text & code, so you can train (actually) open LLMs<br><br>and data acquisition is one of the more expensive & complex aspects of training an LLM, so hopefully we see an acceleration <br><br><a href="https://huggingface.co/blog/Pclanglais/two-trillion-tokens-open" target="_blank" rel="noopener">huggingface.co/blog/Pclangl...</a></div>
<a href="https://huggingface.co/blog/Pclanglais/two-trillion-tokens-open" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">huggingface.co</div>
<div class="embed-title">Releasing the largest multilingual open pretraining dataset</div>
<div class="embed-description">A Blog post by Pierre-Carl Langlais on Hugging Face</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>21</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>5</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">minor complaint: how can you make bold claims about number of tokens when you’re agnostic to the LLM? seems like the actual token count is going to vary based on tokenizer. but anyway, point taken, 2x10^12 ish</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>4</span>
</div>
</div>