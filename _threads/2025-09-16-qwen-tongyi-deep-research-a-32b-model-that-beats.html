---
layout: thread
title: "Qwen Tongyi Deep Research"
date: 2025-09-16 22:37:18 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lyyf3njho22t
likes: 25
reposts: 3
post_count: 5
summary: "Qwen Tongyi Deep Research  a 32B model that beats other SOTA deep research on many benchmarks   tongyi-agent.github.io/blog/introdu..."
similar:
  - url: "/threads/2025-08-06-qwen3-4b-instruct-thinking-uuuh-guys-this-isn/"
    title: "Qwen3-4B Instruct & Thinking"
  - url: "/threads/2025-10-27-minimax-open-sources-m2-this-model-has-been-shaki/"
    title: "MiniMax open sources M2"
  - url: "/threads/2025-09-11-qwen3-next-80b-a3b-base-instruct-thinking-pe/"
    title: "Qwen3-Next-80B-A3B Base, Instruct & Thinking"
---
<div class="thread-post">
<div class="post-text">Qwen Tongyi Deep Research<br><br>a 32B model that beats other SOTA deep research on many benchmarks <br><br><a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/" target="_blank" rel="noopener">tongyi-agent.github.io/blog/introdu...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidhwxqzx5pqixwqwha6hs6pjqe2rgvmk2ankyz4l3sjsv6m2nyddu@jpeg" alt="The image shows eight grouped bar charts comparing benchmark performance across different models. Each subplot corresponds to a benchmark, with models indicated by their logos or text labels. Bars are colored purple (Qwen3.5-Dragon-32B-DeepSearch) and gray/black for other models. Scores are reported as Average F1 or Average@3 depending on the benchmark.

â¸»

Benchmarks and Results:

1. Humanityâ€™s Last Exam
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 32.9
	â€¢	DeepSeek: 29.8
	â€¢	Kimi: 26.9
	â€¢	Gemini 2.5: 26.9
	â€¢	Claude: 21.2
	â€¢	Others lower

2. BrowseComp
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 51.5
	â€¢	DeepSeek: 43.4
	â€¢	Others range 30.0 â†’ 12.2

3. BrowseComp-ZH
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 46.7
	â€¢	DeepSeek: 42.9
	â€¢	Claude: 37.5
	â€¢	Kimi: 29.1
	â€¢	Claude Sonnet: 28.8

4. WebWalkerQA
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 72.2
	â€¢	DeepSeek: 71.7
	â€¢	Claude: 65.6
	â€¢	Kimi: 63.0
	â€¢	GPT-4.1: 61.7
	â€¢	Claude 3.5 Sonnet: 61.2

5. GAIA
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 70.9
	â€¢	Claude: 68.3
	â€¢	DeepSeek: 67.4
	â€¢	GLM-4.5: 66.0
	â€¢	GPT-4.1: 63.1
	â€¢	Kimi: 57.7

6. xbench-DeepSearch
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 75.0
	â€¢	DeepSeek: 71.0
	â€¢	GLM-4.5: 70.0
	â€¢	Kimi: 67.0
	â€¢	Claude Sonnet: 65.0

7. FRAMES
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 90.6
	â€¢	DeepSeek: 83.7
	â€¢	GPT-4.1: 83.0
	â€¢	GLM-4.5: 78.9
	â€¢	Kimi: 78.8
	â€¢	Claude Sonnet: 72.0

8. SimpleQA
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 98.6
	â€¢	Kimi: 93.6
	â€¢	DeepSeek: 93.5
	â€¢	GPT-4.1: 88.3
	â€¢	Gemini 2.5: 55.1
	â€¢	Others ~50

â¸»

Overall:
Qwen3.5-Dragon-32B-DeepSearch (purple bars) consistently outperforms or matches all other models across benchmarks, especially dominating FRAMES (90.6) and SimpleQA (98.6)." class="post-image" loading="lazy">
<div class="image-alt">The image shows eight grouped bar charts comparing benchmark performance across different models. Each subplot corresponds to a benchmark, with models indicated by their logos or text labels. Bars are colored purple (Qwen3.5-Dragon-32B-DeepSearch) and gray/black for other models. Scores are reported as Average F1 or Average@3 depending on the benchmark.

â¸»

Benchmarks and Results:

1. Humanityâ€™s Last Exam
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 32.9
	â€¢	DeepSeek: 29.8
	â€¢	Kimi: 26.9
	â€¢	Gemini 2.5: 26.9
	â€¢	Claude: 21.2
	â€¢	Others lower

2. BrowseComp
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 51.5
	â€¢	DeepSeek: 43.4
	â€¢	Others range 30.0 â†’ 12.2

3. BrowseComp-ZH
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 46.7
	â€¢	DeepSeek: 42.9
	â€¢	Claude: 37.5
	â€¢	Kimi: 29.1
	â€¢	Claude Sonnet: 28.8

4. WebWalkerQA
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 72.2
	â€¢	DeepSeek: 71.7
	â€¢	Claude: 65.6
	â€¢	Kimi: 63.0
	â€¢	GPT-4.1: 61.7
	â€¢	Claude 3.5 Sonnet: 61.2

5. GAIA
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 70.9
	â€¢	Claude: 68.3
	â€¢	DeepSeek: 67.4
	â€¢	GLM-4.5: 66.0
	â€¢	GPT-4.1: 63.1
	â€¢	Kimi: 57.7

6. xbench-DeepSearch
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 75.0
	â€¢	DeepSeek: 71.0
	â€¢	GLM-4.5: 70.0
	â€¢	Kimi: 67.0
	â€¢	Claude Sonnet: 65.0

7. FRAMES
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 90.6
	â€¢	DeepSeek: 83.7
	â€¢	GPT-4.1: 83.0
	â€¢	GLM-4.5: 78.9
	â€¢	Kimi: 78.8
	â€¢	Claude Sonnet: 72.0

8. SimpleQA
	â€¢	Qwen3.5-Dragon-32B-DeepSearch: 98.6
	â€¢	Kimi: 93.6
	â€¢	DeepSeek: 93.5
	â€¢	GPT-4.1: 88.3
	â€¢	Gemini 2.5: 55.1
	â€¢	Others ~50

â¸»

Overall:
Qwen3.5-Dragon-32B-DeepSearch (purple bars) consistently outperforms or matches all other models across benchmarks, especially dominating FRAMES (90.6) and SimpleQA (98.6).</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>25</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">- fully synthetic <br>- continued pre-training (mid) on agent traces<br><br>iâ€™ve been saying this! data! data! data!</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreigtspjwoq6flzqjhf5qln5r6canejhelp4ykoeh2xovo6xv3ibmeq@jpeg" alt="Continual Pre-training a nd Post-training Empowered by Fully Synthetic Data
The model's capabilities are built upon a novel, multi-stage data strategy designed to cre ate vast and high-quality agentic training dat a without relying on costly human annotation." class="post-image" loading="lazy">
<div class="image-alt">Continual Pre-training a nd Post-training Empowered by Fully Synthetic Data
The model's capabilities are built upon a novel, multi-stage data strategy designed to cre ate vast and high-quality agentic training dat a without relying on costly human annotation.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">this is for dataset construction ðŸ¤¯</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreihlkdcaa4lhuwpi2dq3i2mzfq4mxp4y5rjnyvnxivupck3j7kqdi4@jpeg" alt="To address complex, high-uncertainty questi ons, we synthesize web-based QA data thro ugh a novel pipeline. The process begins by constructing a highly interconnected know ledge graph via random walks and isomor phic tables towards tabular data fusion fro m real-world websites, ensuring a realistic in formation structure. We then sample subgra phs and subtables to generate initial questio ns and answers. The crucial step involves int entionally increasing difficulty by strategicall y obfuscating or blurring information within t he question. This practical approach is groun ded in a complete theoretical framework, wh ere we formally model QA difficulty as a serie s of controllable &quot;atomic operations&quot; (e.g., m erging entities with similar attributes) on entit y relationships, allowing us to systematically i ncrease complexity." class="post-image" loading="lazy">
<div class="image-alt">To address complex, high-uncertainty questi ons, we synthesize web-based QA data thro ugh a novel pipeline. The process begins by constructing a highly interconnected know ledge graph via random walks and isomor phic tables towards tabular data fusion fro m real-world websites, ensuring a realistic in formation structure. We then sample subgra phs and subtables to generate initial questio ns and answers. The crucial step involves int entionally increasing difficulty by strategicall y obfuscating or blurring information within t he question. This practical approach is groun ded in a complete theoretical framework, wh ere we formally model QA difficulty as a serie s of controllable &quot;atomic operations&quot; (e.g., m erging entities with similar attributes) on entit y relationships, allowing us to systematically i ncrease complexity.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">from github: they describe how they synthesize successful single step traces, and then combine them together into successful multi-step traces<br><br>ðŸ¤¯<br><br>generally, long traces = higher quality <br><br>they can sythesize their way into the highest quality<br><br><a href="https://github.com/Alibaba-NLP/DeepResearch/tree/main/Agent/AgentFounder" target="_blank" rel="noopener">github.com/Alibaba-NLP/...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreicpjzhmts4eh25vxwyltccquvtb24bchx6uzg4b27g62x76mt3rei@jpeg" alt="FAS - Reasoning Action
Synthesis
By combining questions with their knowledge sources, we emulate the process of deriving final answers through logical inference under fully informed conditions, strengthening the agent's reasoning capability.
HAS â€” Decision-Making
Action Synthesis
We reformulate the agent trajectories as multi-step decision-making processes, fully exploring the reasoning-action space at each step.
HAS expands the agent's capacity to explore the action-answer space while enhancing its decision-making abilities.
Original Trajectory
â€¢ R,
Step-level Scaling,
Question
Solution [31
High ceder Action Synthesis Trajectory
Solution (21
is [Correct)" class="post-image" loading="lazy">
<div class="image-alt">FAS - Reasoning Action
Synthesis
By combining questions with their knowledge sources, we emulate the process of deriving final answers through logical inference under fully informed conditions, strengthening the agent's reasoning capability.
HAS â€” Decision-Making
Action Synthesis
We reformulate the agent trajectories as multi-step decision-making processes, fully exploring the reasoning-action space at each step.
HAS expands the agent's capacity to explore the action-answer space while enhancing its decision-making abilities.
Original Trajectory
â€¢ R,
Step-level Scaling,
Question
Solution [31
High ceder Action Synthesis Trajectory
Solution (21
is [Correct)</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">btw i fucked up the labeling earlier. this is a 30B-A3B, so yeah, extremely compact and cheap</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>3</span>
</div>
</div>