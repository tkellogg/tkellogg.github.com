---
layout: thread
title: "ThinkyMachines: Tinker LoRa training API"
date: 2025-10-01 21:05:11 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3m25wwqxulk2b
likes: 22
reposts: 3
post_count: 6
summary: "ThinkyMachines: Tinker LoRa training API  ThinkingMachines announced their first product, telegraphed by a highly detailed blog earlier this week that..."
similar:
  - url: "/threads/2025-07-06-new-3-token-attention-reduces-pre-training-data-re/"
    title: "new 3-token attention reduces pre-training data requirements"
  - url: "/threads/2025-11-07-notable-they-ripped-out-the-silicon-that-supports/"
    title: "notable: they ripped out the silicon that supports training"
  - url: "/threads/2025-05-14-attentioninfluence-for-pretraining-data-selection/"
    title: "AttentionInfluence: for pretraining data selection"
---
<div class="thread-post">
<div class="post-text">ThinkyMachines: Tinker LoRa training API<br><br>ThinkingMachines announced their first product, telegraphed by a highly detailed blog earlier this week that gave legitimacy to LoRA training<br><br>idea: LoRA works really well for most companies, so they make it easy to train too<br><br><a href="https://thinkingmachines.ai/tinker/" target="_blank" rel="noopener">thinkingmachines.ai/tinker/</a></div>
<a href="https://thinkingmachines.ai/tinker/" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">thinkingmachines.ai</div>
<div class="embed-title">Tinker</div>
<div class="embed-description">Tinker is a training API for researchers and developers.</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>22</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">The Blog! let’s break it down!<br><br>gist: LoRA is just as good as Full Finetuning (FullFT) as long as your data is small and you’re not doing pretraining <br><br>it works extremely well for RL, which should make sense, RL is very sparse on rewards <br><br><a href="https://thinkingmachines.ai/blog/lora/" target="_blank" rel="noopener">thinkingmachines.ai/blog/lora/</a></div>
<a href="https://thinkingmachines.ai/blog/lora/" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">thinkingmachines.ai</div>
<div class="embed-title">LoRA Without Regret</div>
<div class="embed-description">How LoRA matches full training performance more broadly than expected.</div>
</div>
</a>
</div>
<div class="thread-post">
<div class="post-text">LoRA works best when applied to all parts of the model<br><br>i.e. attention-only doesn’t work well<br><br>for MoE, that means you need training data that exercises all experts, which makes MoE quite a bit harder</div>
</div>
<div class="thread-post">
<div class="post-text">Hyperparameters: Rank<br><br>the higher rank the bigger capacity. And yes, it absolutely can approach FullFT, especially on small data</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreiccs3r3hmwse4plq726r4prwhds6g6fwrrar34xlqit2xfkkrqn2q@jpeg" alt="The image shows Figure 1: LoRA training curves for various ranks on Tulu3 and OpenThoughts3 datasets.

It contains four line plots, each with Test NLL (negative log-likelihood) on the y-axis and Step (log scale) on the x-axis. Multiple curves are shown, color-coded by Rank (legend on the right): 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, and full.

⸻

Top row:
	•	Top-left (Llama 3.1 8B, Tulu3 Dataset):
Test NLL steadily decreases with more steps. Higher ranks (closer to full training) achieve lower NLL, while lower ranks flatten out earlier.
	•	Top-right (Llama 3.1 8B, OpenThoughts Dataset):
Similar trend: loss decreases logarithmically with steps. Higher-rank LoRAs closely match FullFT, while low-rank ones diverge sooner.

⸻

Bottom row:
	•	Bottom-left (Llama 3.2 1B, Tulu3 Dataset):
High-rank LoRAs closely match or outperform FullFT, with lower ranks diverging earlier.
	•	Bottom-right (Llama 3.2 1B, OpenThoughts Dataset):
High-rank LoRA performs worse than FullFT on this dataset, showing dataset-specific variation.

⸻

Caption summary:
	•	FullFT and high-rank LoRAs show similar learning curves, with loss decreasing roughly linearly with log-steps.
	•	Lower-rank LoRAs plateau earlier, showing lack of capacity.
	•	For the 1B model (bottom plots): high-rank LoRA outperforms FullFT on Tulu3 but underperforms on OpenThoughts.
	•	This suggests dataset-specific variation in LoRA effectiveness, due to different training dynamics or generalization behavior." class="post-image" loading="lazy">
</div>
</div>
</div>
<div class="thread-post">
<div class="post-text">Batch size: not too big, loss can degrade fast<br><br>Attention-only underperforms MLP-only consistently</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">RL on LoRA can match FullFT, even on ranks as low as 1<br><br>THIS IS HUGE<br><br>if you’ve been thinking about RL or RL environments, you should absolutely be thinking about LoRA. it would be idiotic to not consider it<br><br>also: LoRAs stack, so all these RL environments can be shareable in new ways</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>2</span>
</div>
</div>