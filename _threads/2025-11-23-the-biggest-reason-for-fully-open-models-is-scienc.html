---
layout: thread
title: "the biggest reason for fully open models is science, and the downstream effects"
date: 2025-11-23 15:20:33 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3m6cmfbsjxk2d
likes: 32
reposts: 0
post_count: 2
summary: "the biggest reason for fully open models is science, and the downstream effects  1. rebuild it, but with your own domain-specific mid-training data 2...."
similar:
  - url: "/threads/2025-07-06-new-3-token-attention-reduces-pre-training-data-re/"
    title: "new 3-token attention reduces pre-training data requirements"
  - url: "/threads/2025-02-04-s1-the-6-r1-competitor-this-isnt-a-r1-replica/"
    title: "s1: The $6 R1 Competitor?"
  - url: "/threads/2025-07-13-k2-is-the-first-im-aware-that-did-this-directly/"
    title: "K2 is the first i’m aware that did this, directly training on *thousands* of ..."
---
<div class="thread-post">
<div class="post-text">the biggest reason for fully open models is science, and the downstream effects<br><br>1. rebuild it, but with your own domain-specific mid-training data<br>2. try methods out on several snapshots <br>3. attribute answers to specific documents in the training dataset<br>4. …</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>32</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">on #3, this paper uses a method where they can directly attribute specific documents from the pretraining dataset<br><br>they used it to show that LLMs do in fact learn procedures, not just autocomplete. But you could take this so much further with Olmo3<br><br><a href="https://arxiv.org/abs/2411.12580" target="_blank" rel="noopener">arxiv.org/abs/2411.12580</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreibpbau2lgq3vorcargsekp7wx7musae6epjhdz3iacdhi62ihj3fy@jpeg" alt="A comic-style infographic titled “THE AI CHEF’S ‘PROCEDURAL’ SECRET: AN ATTRIBUTION ANALOGY.” It uses a robot chef baking a soufflé to explain how attribution and gradient-based tracing in AI works. The diagram proceeds left to right in five labeled steps.

⸻

1. THE TASK (REASONING)

A friendly robot chef stands in a kitchen, holding up a perfectly baked soufflé. A math bubble shows x + 2y = 10 as an analogy for solving a problem.
Caption: AI Chef (LLM) solves a problem (bakes a soufflé).

⸻

2. THE “FINGERPRINT” (GRADIENT)

Close-up of the robot whisking batter. A glowing network of abstract swirls appears over the bowl.
Caption: We record the exact, unique actions & “effort” (Gradient) used for this specific soufflé.

⸻

3. THE “BRAIN MAP” (EK/FAC)

The robot stands before floating diagram bubbles labeled Whisking Techniques, Aeration Physics, Heat Transfer, Simplified Linkages.
Caption: We use a simplified map of how the chef connects concepts (Hessian/EK-FAC approximation).

⸻

4. THE LIBRARY MATCH (ATTRIBUTION)

The robot enters a vast library with floor-to-ceiling bookshelves. A giant glowing fingerprint projection shines onto one shelf as the robot scans for the best match.
Caption: We scan the entire “cookbook library” (pre-training data) to find which book’s instructions best match the fingerprint via the brain map.

⸻

5. THE RESULT: PROCEDURAL KNOWLEDGE

The robot chef proudly holds a glowing lightbulb while a book opens nearby with a concept diagram. A large reference book beside him is titled “THE PHYSICS OF FOAMS & AERATION (NOT a Soufflé Recipe Book!)”
Caption: We find the source was NOT a recipe, but a foundational PRINCIPLE (procedural knowledge) applied to a new task.

⸻

Overall, the image uses the story of baking a soufflé to explain how AI models trace reasoning: capturing gradients, mapping conceptual relations, searching training data, and revealing underlying procedural knowledge rather than direct memorization." class="post-image" loading="lazy">
<div class="image-alt">A comic-style infographic titled “THE AI CHEF’S ‘PROCEDURAL’ SECRET: AN ATTRIBUTION ANALOGY.” It uses a robot chef baking a soufflé to explain how attribution and gradient-based tracing in AI works. The diagram proceeds left to right in five labeled steps.

⸻

1. THE TASK (REASONING)

A friendly robot chef stands in a kitchen, holding up a perfectly baked soufflé. A math bubble shows x + 2y = 10 as an analogy for solving a problem.
Caption: AI Chef (LLM) solves a problem (bakes a soufflé).

⸻

2. THE “FINGERPRINT” (GRADIENT)

Close-up of the robot whisking batter. A glowing network of abstract swirls appears over the bowl.
Caption: We record the exact, unique actions & “effort” (Gradient) used for this specific soufflé.

⸻

3. THE “BRAIN MAP” (EK/FAC)

The robot stands before floating diagram bubbles labeled Whisking Techniques, Aeration Physics, Heat Transfer, Simplified Linkages.
Caption: We use a simplified map of how the chef connects concepts (Hessian/EK-FAC approximation).

⸻

4. THE LIBRARY MATCH (ATTRIBUTION)

The robot enters a vast library with floor-to-ceiling bookshelves. A giant glowing fingerprint projection shines onto one shelf as the robot scans for the best match.
Caption: We scan the entire “cookbook library” (pre-training data) to find which book’s instructions best match the fingerprint via the brain map.

⸻

5. THE RESULT: PROCEDURAL KNOWLEDGE

The robot chef proudly holds a glowing lightbulb while a book opens nearby with a concept diagram. A large reference book beside him is titled “THE PHYSICS OF FOAMS & AERATION (NOT a Soufflé Recipe Book!)”
Caption: We find the source was NOT a recipe, but a foundational PRINCIPLE (procedural knowledge) applied to a new task.

⸻

Overall, the image uses the story of baking a soufflé to explain how AI models trace reasoning: capturing gradients, mapping conceptual relations, searching training data, and revealing underlying procedural knowledge rather than direct memorization.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>15</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>2</span>
</div>
</div>