---
layout: thread
title: "When two LLMs debate, both think they’ll win"
date: 2025-06-10 22:59:20 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lrbymssetk2i
likes: 78
reposts: 19
post_count: 4
summary: "When two LLMs debate, both think they’ll win  Absolutely fascinating paper shows that LLMs basically cannot judge their own performance. None of the p..."
similar:
  - url: "/threads/2025-05-01-there-is-legions-of-misunderstandings-around-llms/"
    title: "there is legions of misunderstandings around LLMs"
  - url: "/threads/2025-11-02-llms-can-report-their-own-experience-the-most-con/"
    title: "LLMs can report their own experience"
  - url: "/threads/2025-06-28-llms-will-never-be-able-to-reason-because-reason/"
    title: "“LLMs will NEVER be able to reason, because reasoning must be consistent but ..."
---
<div class="thread-post">
<div class="post-text">When two LLMs debate, both think they’ll win<br><br>Absolutely fascinating paper shows that LLMs basically cannot judge their own performance. None of the prompting techniques worked<br><br><a href="https://arxiv.org/abs/2505.19184" target="_blank" rel="noopener">arxiv.org/abs/2505.19184</a></div>
<a href="https://arxiv.org/abs/2505.19184" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">arxiv.org</div>
<div class="embed-title">When Two LLMs Debate, Both Think They'll Win</div>
<div class="embed-description">Can LLMs accurately adjust their confidence when facing opposition? Building on previous studies measuring calibration on static fact-based question-answering tasks, we evaluate Large Language Models ...</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>78</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>19</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">Humans are also overconfident, but they adjust that confidence much more often than LLMs</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>6</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">RLHF exacerbates overconfidence <br><br>i.e. when we train LLMs to be more like us, that’s when the overconfidence gets introduced <br><br>again, LLMs are a mirror into society</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreidanrm7glfxnczjwx2n5zov4qy2cgddaxmaoq7roq7xj6fkq6hpii@jpeg" alt="RLHF amplification: Post-training for human preferences exacerbates overconfidence, biasing models to indicate high certainty even when incorrect (Leng et al., 2025) and provide more 7/10 ratings (West and Potts, 2025; Ope-nAl et al., 2024) relative to base models. Tjuat-ja et al. (2024) found mild correlation between uncertainty and LLMs exhibiting certain hu-man-like response bias (r=0.259 for RLHF and r=0.267 for base models), but less so compared to humans (r=0.4-0.6). This suggests that LLM overconfidence increases human-like response bias, but human-like response bias itself does not cause overconfidence." class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>10</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">The entire discussion section is a ride<br><br>obviously this is very important to be aware of when building agents</div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreig62dwiaq66sitbwdp6ys4bjrlb272yvi2ialnauojzjcafmopgdi@jpeg" alt="5 Discussion
5.1 Metacognitive Limitations and Possible Explanations
Our findings reveal significant limitations in LLMs' metacognitive abilities to assess argumentative positions and revise confidence in an adversarial debate context. This threatens assistant applications (where users may accept confidently-stated but incorrect outputs without verification) and agentic deployments (where systems must revise their reasoning and solutions based on new information in dynamically changing environments). Existing literature provides several explanations for LLM over-confidence, including human-like biases and LLM-specific factors:" class="post-image" loading="lazy">
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>7</span>
</div>
</div>