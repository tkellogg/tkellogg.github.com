---
layout: thread
title: "NVIDIA’s moat just got bigger"
date: 2025-09-12 12:05:37 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lyn7wgrgvk2y
likes: 21
reposts: 1
post_count: 2
summary: "NVIDIA’s moat just got bigger  the Rubin CTX is for inference, and it’s a beast  sold as a rack as the unit, this one optimizes various LLM phases int..."
similar:
  - url: "/threads/2025-09-02-nvidia-stock-is-about-to-surge-a-vldb-paper-from/"
    title: "NVIDIA stock is about to surge"
  - url: "/threads/2025-08-16-gpt-5-is-massively-better-at-offensive-cybersecuri/"
    title: "GPT-5 is massively better at offensive cybersecurity (hacking & pen testing) "
  - url: "/threads/2025-11-07-notable-they-ripped-out-the-silicon-that-supports/"
    title: "notable: they ripped out the silicon that supports training"
---
<div class="thread-post">
<div class="post-text">NVIDIA’s moat just got bigger<br><br>the Rubin CTX is for inference, and it’s a beast<br><br>sold as a rack as the unit, this one optimizes various LLM phases into silicon<br><br>notably: it does prefill in fp4 with low memory bandwidth and huge compute<br><br><a href="https://semianalysis.com/2025/09/10/another-giant-leap-the-rubin-cpx-specialized-accelerator-rack/" target="_blank" rel="noopener">semianalysis.com/2025/09/10/a...</a></div>
<a href="https://semianalysis.com/2025/09/10/another-giant-leap-the-rubin-cpx-specialized-accelerator-rack/" class="post-embed" target="_blank" rel="noopener">
<div class="embed-content">
<div class="embed-domain">semianalysis.com</div>
<div class="embed-title">Another Giant Leap: The Rubin CPX Specialized Accelerator & Rack</div>
<div class="embed-description">Nvidia announced the Rubin CPX, a solution that is specifically designed to be optimized for the prefill phase, with the single-die Rubin CPX heavily emphasizing compute FLOPS over memory bandwidth…</div>
</div>
</a>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>21</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>1</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">there’s always been someone out there saying, “yeah! just do the transformer directly in hardware”<br><br>that’s not what this is. this merely provides transistor layouts such that it’s really easy to make LLMs go screaming fast <br><br>you could still do scientific compute on them, albeit not in fp4</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>3</span>
</div>
</div>