---
layout: thread
title: "Mistral Small 3"
date: 2025-01-30 21:53:55 +0000
bluesky_url: https://bsky.app/profile/did:plc:ckaz32jwl6t2cno6fmuw2nhn/post/3lgyhvarzo22b
likes: 23
reposts: 3
post_count: 2
summary: "Mistral Small 3  A 24B LLM that's VERY fast with great function calling  More important, MISTRAL IS OPEN SOURCE AGAIN!!!!!!  mistral.ai/news/mistral....."
similar:
  - url: "/threads/2025-09-10-llms-are-deterministic-no-theyre-not-well/"
    title: "“LLMs are deterministic!”"
  - url: "/threads/2024-12-14-readable-paper-alert-blt-what-if-we-just-g/"
    title: "⚠️ Readable Paper Alert ⚠️"
  - url: "/threads/2024-12-05-this-feels-very-big-traditional-weather-forecast/"
    title: "This feels very big "
---
<div class="thread-post">
<div class="post-text">Mistral Small 3<br><br>A 24B LLM that's VERY fast with great function calling<br><br>More important, MISTRAL IS OPEN SOURCE AGAIN!!!!!!<br><br><a href="https://mistral.ai/news/mistral-small-3/" target="_blank" rel="noopener">mistral.ai/news/mistral...</a></div>
<div class="post-images single">
<div class="post-image-container">
<img src="https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:ckaz32jwl6t2cno6fmuw2nhn/bafkreiehug44xvgpywk3vbs7k753mszs3otmwymuktpmanszzfx3qnendy@jpeg" alt="A scatter plot comparing AI model performance on MMLU-Pro against latency in milliseconds per token. The x-axis represents latency (milliseconds per token), and the y-axis represents performance (MMLU-Pro score). 

- **Mistral Small 3** (highlighted in orange with a castle emoji) is positioned in the upper-left region, indicating high performance and low latency.
- **GPT-4o Mini** is slightly lower in performance but has higher latency.
- **Qwen-2.5 32B** is positioned higher in performance but with greater latency.
- **Gemma-2 27B** has lower performance and the highest latency among the models.

The benchmark is based on Apache 2.0 models using vLLM with a batch size of 16 on 4xH100 GPUs, with GPT-4o Mini data sourced from OpenAI's API." class="post-image" loading="lazy">
<div class="image-alt">A scatter plot comparing AI model performance on MMLU-Pro against latency in milliseconds per token. The x-axis represents latency (milliseconds per token), and the y-axis represents performance (MMLU-Pro score). 

- **Mistral Small 3** (highlighted in orange with a castle emoji) is positioned in the upper-left region, indicating high performance and low latency.
- **GPT-4o Mini** is slightly lower in performance but has higher latency.
- **Qwen-2.5 32B** is positioned higher in performance but with greater latency.
- **Gemma-2 27B** has lower performance and the highest latency among the models.

The benchmark is based on Apache 2.0 models using vLLM with a batch size of 16 on 4xH100 GPUs, with GPT-4o Mini data sourced from OpenAI's API.</div>
</div>
</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>23</span>
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M7 7h10v3l4-4-4-4v3H5v6h2V7zm10 10H7v-3l-4 4 4 4v-3h12v-6h-2v4z"/></svg>3</span>
</div>
</div>
<div class="thread-post">
<div class="post-text">i had mistral-small produce an economic model for a 25% tariff on Mexico and fed the answer into R1 and R1 had A LOT to say, mistral did not hold up to R1’s high standards</div>
<div class="post-stats">
<span class="stat-item"><svg class="stat-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 21.35l-1.45-1.32C5.4 15.36 2 12.28 2 8.5 2 5.42 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.09C13.09 3.81 14.76 3 16.5 3 19.58 3 22 5.42 22 8.5c0 3.78-3.4 6.86-8.55 11.54L12 21.35z"/></svg>2</span>
</div>
</div>